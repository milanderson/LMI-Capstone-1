{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rule Splitting & ElasticSearch Uploads, Queries\n",
    "### LMI Capstone Team\n",
    "### Summer Chambers | Steve Morris | Kaleb Shikur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import elasticsearch\n",
    "from elasticsearch import Elasticsearch, RequestsHttpConnection, ElasticsearchException\n",
    "import regex as re\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import requests #gets urls\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "import sys\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 173.79.72.92, port 9200\n",
    "# host = '173.79.72.92'\n",
    "endpoint = 'https://search-lmi-capstone-2-525zkk33t4z5iy6ozqd63ctgmq.us-east-1.es.amazonaws.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'eb7b3348d3c34ce7dc22f263930c51ad', 'cluster_name': '846033058400:lmi-capstone-2', 'cluster_uuid': 'QzTffcNgStmet053IRSP2w', 'version': {'number': '7.9.1', 'build_flavor': 'oss', 'build_type': 'tar', 'build_hash': 'unknown', 'build_date': '2020-11-03T09:54:32.349659Z', 'build_snapshot': False, 'lucene_version': '8.6.2', 'minimum_wire_compatibility_version': '6.8.0', 'minimum_index_compatibility_version': '6.0.0-beta1'}, 'tagline': 'You Know, for Search'}\n"
     ]
    }
   ],
   "source": [
    "#es = Elasticsearch(endpoint, timeout = 45)\n",
    "es = Elasticsearch(endpoint, timeout=600, max_retries=2, retry_on_timeout=True)\n",
    "\n",
    "\n",
    "\n",
    "print(es.info())\n",
    "\n",
    "# es = Elasticsearch(\n",
    "#     hosts=[{'host': host, 'port': 443}],\n",
    "#     http_auth=auth,\n",
    "#     use_ssl=True,\n",
    "#     verify_certs=True,\n",
    "#     connection_class=RequestsHttpConnection\n",
    "# )\n",
    "# print(es.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Rule Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_url = \"https://mikeanders.org/data/CMS/CMS-2018-0101-0001/Rule/CMS-2018-0101-0001.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitRule_headers(rule_url):\n",
    "\n",
    "    alltxt = requests.get(rule_url).text.lower()#.encode('unicode_escape').decode() #encodes like raw strings\n",
    "    \n",
    "    #Isolate Section 2\n",
    "    initialsplit = alltxt.split(\"ii. provisions of the proposed regulations\") #split before section 2\n",
    "    sec2andon = initialsplit[1] #choose latter half\n",
    "    sec2list = sec2andon.split(\"iii. collection of information requirements\") #split before section 3\n",
    "    splitlist = sec2list[0] #choose first half\n",
    "    \n",
    "    rulechunks = []\n",
    "    \n",
    "    startdict = {'a2':['2. proposals for modified participation options under 5-year agreement periods', '3. creating a basic track with glide path to performance-based risk'], \\\n",
    "     'a3':['3. creating a basic track with glide path to performance-based risk', '4. permitting annual participation elections'], \\\n",
    "    'a4b':['b. proposals for permitting election of differing levels of risk within the basic track\\'s glide path', 'c. proposals for permitting annual election of beneficiary assignment methodology'], \\\n",
    "    'a4c':['c. proposals for permitting annual election of beneficiary assignment methodology', '5. determining participation options based on medicare ffs revenue and prior participation '], \\\n",
    "    'a5b':['b. differentiating between low revenue acos and high revenue acos', 'c. determining participation options based on prior participation of aco legal entity and aco participants'], \\\n",
    "    'a5c':['c. determining participation options based on prior participation of aco legal entity and aco participants', 'd. monitoring for financial performance'], \\\n",
    "    'a5d': ['d. monitoring for financial performance', '6. requirements for aco participation in two-sided models'], \\\n",
    "    'a6b': ['b. election of msr/mlr by acos', 'c. aco repayment mechanisms'], \\\n",
    "    'a6c': ['c. aco repayment mechanisms', 'd. advance notice for and payment consequences of termination '], \\\n",
    "    'a6d2': ['(2) proposals for advance notice of voluntary termination', '(3) proposals for payment consequences of termination'], \\\n",
    "    'a6d3': ['(3) proposals for payment consequences of termination', '7. participation options for agreement periods beginning in 2019'], \\\n",
    "    'a7b': ['b. methodology for determining financial and quality performance for the 6-month performance years during 2019', 'c. applicability of program policies to acos participating in a 6-month performance year'], \\\n",
    "    'a7c': ['c. applicability of program policies to acos participating in a 6-month performance year', 'b. fee-for-service benefit enhancements'], \\\n",
    "    'b2a': ['a. shared savings program snf 3-day rule waiver', 'b. billing and payment for telehealth services'], \\\n",
    "    'b2b': ['b. billing and payment for telehealth services', 'c. providing tools to strengthen beneficiary engagement'], \\\n",
    "    'c2': ['2. beneficiary incentives', '3. empowering beneficiary choice'], \\\n",
    "    'c3a': ['3. empowering beneficiary choice', 'b. beneficiary opt-in based assignment methodology'], \\\n",
    "    'c3b': ['b. beneficiary opt-in based assignment methodology', 'd. benchmarking methodology refinements'],  \\\n",
    "    'd2': ['2. risk adjustment methodology for adjusting historical benchmark each performance year', '3. use of regional factors when establishing and resetting acos\\' benchmarks'], \\\n",
    "    'd3b': ['b. proposals to apply regional expenditures in determining the benchmark for an aco\\'s first agreement period', 'c. proposals for modifying the regional adjustment'], \\\n",
    "    'd3c': ['c. proposals for modifying the regional adjustment', 'd. proposals for modifying the methodology for calculating growth rates used in establishing, resetting, and updating the benchmark'], \\\n",
    "    'd3d': ['d. proposals for modifying the methodology for calculating growth rates used in establishing, resetting, and updating the benchmark', '4. technical changes to incorporate references to benchmark rebasing policies'], \\\n",
    "    'd4': ['4. technical changes to incorporate references to benchmark rebasing policies', 'e. updating program policies'], \\\n",
    "    'e2': ['2. revisions to policies on voluntary alignment', '3. revisions to the definition of primary care services used in beneficiary assignment'], \\\n",
    "    'e3': ['3. revisions to the definition of primary care services used in beneficiary assignment', '4. extreme and uncontrollable circumstances policies for the shared savings program'], \\\n",
    "    'e4': ['4. extreme and uncontrollable circumstances policies for the shared savings program', '5. program data and quality measures'], \\\n",
    "    'e5':['5. program data and quality measures', '6. promoting interoperability'], \\\n",
    "    'e6':['6. promoting interoperability', '7. coordination of pharmacy care for aco beneficiaries'], \\\n",
    "    'e7':['7. coordination of pharmacy care for aco beneficiaries', 'f. applicability of proposed policies to track 1+ model acos'], \\\n",
    "    'f2':['2. unavailability of application cycles for entry into the track 1+ model in 2019 and 2020', '3. applicability of proposed policies to track 1+ model acos through revised program regulations or revisions to track 1+ model participation agreements'], \\\n",
    "    'f3':['3. applicability of proposed policies to track 1+ model acos through revised program regulations or revisions to track 1+ model participation agreements', 'g. summary of proposed timing of applicability']}\n",
    "    \n",
    "    for key, value in startdict.items():    \n",
    "       splitlist = splitlist.split(value[0]) #split on start of desired section\n",
    "       split_further = splitlist[1].split(value[1]) #split again on start of undesired section\n",
    "       rulechunks.append({\"section\": key, \"text\": (value[0]+split_further[0])}) #choose only first half to upload to dict\n",
    "       splitlist = splitlist[1] #choose second half to prepare for next split\n",
    "        \n",
    "    #lengths = [len(chunk) for chunk in rulechunks.values()]\n",
    "\n",
    "    return rulechunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_header_split = splitRule_headers(rule_url)\n",
    "headers_id_list = [doc[\"section\"] for doc in test_header_split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitRule_line_hybrid(rule_url):\n",
    "    new_rule_chunks = []\n",
    "    chunks = splitRule_headers(rule_url)\n",
    "    for doc in chunks:\n",
    "        paragraphs = doc[\"text\"].split('\\r\\n')\n",
    "        #add new lines while under 6000 characters\n",
    "        for i in range(len(paragraphs) - 1):\n",
    "            while i < (len(paragraphs)-1) and len(paragraphs[i]) < 6000:\n",
    "                paragraphs[i] += paragraphs[i+1]\n",
    "                del(paragraphs[i+1])\n",
    "        for i in range(len(paragraphs)):\n",
    "            new_rule_chunks.append({\"section\": doc[\"section\"]+str(i), \"text\": paragraphs[i]})\n",
    "    return new_rule_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = splitRule_line_hybrid(rule_url)\n",
    "lengths = {chunk[\"section\"]: len(chunk[\"text\"]) for chunk in result}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading Rule Splits to ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rulesplit_toES(rulechunks, es_index):\n",
    "    for chunk in rulechunks:\n",
    "            res = es.index(index=es_index, id=chunk[\"section\"], body=chunk, doc_type='_doc')\n",
    "            es.indices.refresh(index=es_index)\n",
    "    print(\"Last id uploaded:\", chunk[\"section\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last id uploaded: f3\n"
     ]
    }
   ],
   "source": [
    "rulesplit_1shard = splitRule_headers(rule_url)\n",
    "upload_1shard = rulesplit_toES(rulesplit_1shard, \"headers_1shard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last id uploaded: f31\n"
     ]
    }
   ],
   "source": [
    "rulesplit_1shard = splitRule_line_hybrid(rule_url)\n",
    "upload_1shard = rulesplit_toES(rulesplit_1shard, \"hybrid_1shard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last id uploaded: f3\n"
     ]
    }
   ],
   "source": [
    "rulesplit_test1 = splitRule_headers(rule_url)\n",
    "test_upload1 = rulesplit_toES(rulesplit_test1, \"headers_custom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last id uploaded: f31\n"
     ]
    }
   ],
   "source": [
    "rulesplit_test2 = splitRule_line_hybrid(rule_url)\n",
    "test_upload2 = rulesplit_toES(rulesplit_test2, \"hybrid_custom\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index Utilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##es.indices.refresh(index=\"index_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "##es.indices.delete(index='index_name', ignore=[400, 404])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve Comment Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_url = \"https://mikeanders.org/data/CMS/CMS-2018-0101-0001/Comments/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_comments(comment_url):\n",
    "    comment_url = \"https://mikeanders.org/data/CMS/CMS-2018-0101-0001/Comments/\"\n",
    "    response = requests.get(comment_url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    a_tags = soup.findAll(\"a\")\n",
    "    links = [tag[\"href\"] for tag in a_tags]\n",
    "    txt_links = [link for link in links if '.txt' in link]\n",
    "    comments = {}\n",
    "    for suffix in txt_links:\n",
    "        comments[suffix] = requests.get(comment_url+suffix).text.lower()\n",
    "        #print(f\"scraping comment {suffix}\")\n",
    "    print(f\"scraped {len(txt_links)} comments\")\n",
    "    return comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments2018 = retrieve_comments(comment_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_lengths = {key: len(value) for key, value in comments2018.items()}\n",
    "sorted_comment_lengths = dict(sorted(comment_lengths.items(), key=lambda x: x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments2018 = {(key[14:20]): value for key, value in comments2018.items()}\n",
    "sorted_keys = sorted(list(comments2018.keys()))\n",
    "\n",
    "# now for each key in the list\n",
    "for i in range(len(comments2018)-1):\n",
    "    # get key at index i and key at index i+1 and compare them\n",
    "    if sorted_keys[i+1][0:4] == sorted_keys[i][0:4]:\n",
    "        comments2018[sorted_keys[i+1]] = comments2018[sorted_keys[i]] + comments2018[sorted_keys[i+1]]\n",
    "        del(comments2018[sorted_keys[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments2018 = {key[0:4]:value for key, value in comments2018.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(comments2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to .json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_json = json.dumps(comments2018)\n",
    "f = open('comments2018.json','w')\n",
    "f.write(comment_json)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read .json (avoid scraping comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('comments2018.json') as f:\n",
    "    comments2018 = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove duplicate comments, comments less than 30 characters long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0083': 'good luck everyone!',\n",
       " '0142': 'see attached file(s)',\n",
       " '0237': 'see attached file(s)\\n',\n",
       " '0270': 'see attached file(s)\\n',\n",
       " '0293': 'see attached file(s)',\n",
       " '0321': 'see attached file(s)\\n\\n',\n",
       " '0324': 'see attachement\\n',\n",
       " '0329': 'see attached file(s)\\n',\n",
       " '0342': 'see attached file(s)\\n',\n",
       " '0447': 'see attached file(s)'}"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_comments2018 = {key: val for key, val in comments2018.items() if len(val) >= 30}\n",
    "short_ones = {key: val for key, val in comments2018.items() if len(val) < 30}\n",
    "list_reduced_comments2018 = list(reduced_comments2018.values())\n",
    "len(reduced_comments2018)\n",
    "short_ones #these seem okay to remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DeDuper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = getDupes(list_reduced_comments2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "print(len(duplicates))\n",
    "#duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "dupdict = {}\n",
    "for i, dup in enumerate(duplicates):\n",
    "    dupdict[i] = [list_reduced_comments2018[idx][0:120] for idx in dup]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dupdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_deleted = []\n",
    "for num, duplist in enumerate(duplicates):\n",
    "    for idx, comment in enumerate(list_reduced_comments2018):\n",
    "        if idx in duplist[:-1]:\n",
    "            tb_deleted.append(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "363"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tb_deleted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_comments2018 = {key:value for key, value in reduced_comments2018.items() if value not in tb_deleted}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_comments2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to .json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump = json.dumps(unique_comments2018)\n",
    "f = open('unique_comments2018.json','w')\n",
    "f.write(dump)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read .json (avoid scraping comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('unique_comments2018.json') as f:\n",
    "    unique_comments2018 = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split up longer comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_comments = {key:value for key, value in unique_comments2018.items() if len(value) > 5000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded = unique_comments2018.copy()\n",
    "for key, value in long_comments.items():\n",
    "    paragraphs = value.split('\\n')\n",
    "    for i in range(len(paragraphs) - 1):\n",
    "        while i < (len(paragraphs) - 1) and len(paragraphs[i]) < 4800:\n",
    "            paragraphs[i] += paragraphs[i+1]\n",
    "            del(paragraphs[i+1])\n",
    "    for i in range(len(paragraphs)):\n",
    "        expanded[key+'_'+str(i)] = paragraphs[i]\n",
    "expanded = {key:value for key, value in expanded.items() if key not in list(long_comments.keys())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#{key:len(value) for key, value in expanded.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove \\r, \\n, \\s, and any weird or non-ascii characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded = {key:value.replace(\"\\r\", \" \").replace(\"\\n\", \" \").replace(\"\\s\", \" \") for key, value in expanded.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in expanded.items():\n",
    "    expanded[key] = ''.join(c for c in value if c in string.printable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to .json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump = json.dumps(expanded)\n",
    "f = open('expanded.json','w')\n",
    "f.write(dump)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read .json (avoid scraping comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('expanded.json') as f:\n",
    "    expanded = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing other basic API Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check to see if document_id exists in index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es.exists(index=\"headers\", id=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View term vectors for a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_index': 'headers',\n",
       " '_type': '_doc',\n",
       " '_id': '6',\n",
       " '_version': 1,\n",
       " 'found': True,\n",
       " 'took': 0,\n",
       " 'term_vectors': {}}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.termvectors(index=\"headers\", id=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See an explanation for a query's score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es.explain(index=\"hybrid_6000\", id=6, body={\"query\": test_comment0002_query}) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Single query\n",
    "def ES_search(es_index, querydict):\n",
    "    search = es.search(index=es_index, doc_type=\"_doc\", body={\"query\": querydict})\n",
    "    test_dict = {}\n",
    "    if search['hits']['hits'] != []:\n",
    "        for h in search['hits']['hits']:\n",
    "            test_dict[h['_id']]=h['_score']\n",
    "    return test_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test a few types of queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With \"Simple Query String\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_comment0002_query = {\"simple_query_string\": {\"query\": expanded['0002']}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a4c': 151.79832,\n",
       " 'a6c': 148.1676,\n",
       " 'a2': 77.816055,\n",
       " 'a5b': 69.46873,\n",
       " 'a5c': 63.691624,\n",
       " 'a3': 62.509632,\n",
       " 'a5d': 59.190556,\n",
       " 'c3b': 57.04912,\n",
       " 'd3d': 56.666798,\n",
       " 'e4': 55.296684}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ES_search(\"headers_standard\", test_comment0002_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With \"More Like This\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlt_test0002 = {\"more_like_this\": {\"fields\": [\"text\"], \"like\": expanded[\"0004\"]}}#, \"min_term_freq\": 1, \"max_query_terms\": 30}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a7c': 4.9438677,\n",
       " 'a4c': 4.86566,\n",
       " 'a6c': 4.861052,\n",
       " 'a6b': 4.762745,\n",
       " 'a5b': 4.6482067,\n",
       " 'e3': 4.563411,\n",
       " 'a6d3': 4.2192745,\n",
       " 'a3': 4.1510067,\n",
       " 'a4b': 4.104784,\n",
       " 'd3d': 3.6692913}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ES_search(\"headers_standard\", mlt_test0002)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With \"Match\" or \"Match_Phrase\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_test0002 = {\"match\" : {\"text\" : expanded[\"0002\"]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a4c': 152.92398,\n",
       " 'a6c': 149.1905,\n",
       " 'a2': 80.02064,\n",
       " 'a5b': 70.87577,\n",
       " 'a5c': 63.691628,\n",
       " 'a3': 62.509632,\n",
       " 'a5d': 59.190556,\n",
       " 'c3b': 57.04912,\n",
       " 'd3d': 56.666798,\n",
       " 'e4': 55.296684}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ES_search(\"headers_standard\", match_test0002)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Bool, should\" and \"Match\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_test = {\"bool\":{\"should\":[{\"match\":{\"text\":{\"query\":expanded[\"0002\"]}}}]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a6c75': 95.89467,\n",
       " 'a20': 90.73644,\n",
       " 'e51': 89.76659,\n",
       " 'a4c105': 86.93875,\n",
       " 'a6c42': 81.72023,\n",
       " 'a4c72': 80.15328,\n",
       " 'a4c106': 77.97759,\n",
       " 'a6c76': 75.50844,\n",
       " 'a6d30': 73.30347,\n",
       " 'a4c53': 72.953026}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ES_search(\"hybrid_reindex\", match_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple Queries in One, Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"query\":\n",
    "{  \n",
    "    \"bool\":{  \n",
    "        \"should\":[  \n",
    "            {  \n",
    "                \"match\":{  \n",
    "                    \"title\":{  \n",
    "                        \"query\":\"cat\",\n",
    "                        \"boost\":2\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {  \n",
    "                \"match\":{  \n",
    "                    \"content\":{  \n",
    "                        \"query\":\"cat\",\n",
    "                        \"boost\":2\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying Many Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_shards': {'total': 10, 'successful': 5, 'failed': 0}}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.indices.refresh(index='headers_reindex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_large_query(comment_dict, es_index):\n",
    "    results = {}\n",
    "    for key, value in comment_dict.items():\n",
    "        query = {\"match\" : {\"text\" : value}}\n",
    "        try:\n",
    "            search1 = ES_search(es_index, query)\n",
    "        except ElasticsearchException as es1:\n",
    "            print(es1)\n",
    "            es.indices.refresh(index=es_index)\n",
    "            continue\n",
    "        results[key] = {section:score for section, score in search1.items()}\n",
    "        time.sleep(3)\n",
    "        es.indices.refresh(index=es_index)\n",
    "        print(key, end=' ')\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0002 0004 0005 "
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'0002': {'a4c': 168.99496,\n",
       "  'a6c': 167.30307,\n",
       "  'a2': 95.26429,\n",
       "  'a5b': 82.7537,\n",
       "  'a5c': 81.27746,\n",
       "  'a3': 77.68761,\n",
       "  'c3b': 74.62591,\n",
       "  'c2': 73.10407,\n",
       "  'd3d': 72.49425,\n",
       "  'a5d': 71.89928},\n",
       " '0004': {'a6c': 122.3792,\n",
       "  'a4c': 121.377754,\n",
       "  'a3': 65.7468,\n",
       "  'a7c': 64.3224,\n",
       "  'a2': 60.67717,\n",
       "  'e4': 60.006237,\n",
       "  'a5b': 58.162373,\n",
       "  'c2': 57.334846,\n",
       "  'e6': 56.327347,\n",
       "  'a5c': 54.380154},\n",
       " '0005': {'a4c': 377.71533,\n",
       "  'a6c': 367.36398,\n",
       "  'a5b': 202.81691,\n",
       "  'b2a': 171.71921,\n",
       "  'b2b': 166.34723,\n",
       "  'c2': 165.0891,\n",
       "  'd3d': 161.18617,\n",
       "  'a5d': 157.70349,\n",
       "  'e4': 154.42505,\n",
       "  'a3': 151.9862}}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test on dictionary of 3 key/value\n",
    "dict_first3 = {}\n",
    "for key in list(expanded.keys())[0:3]:\n",
    "    dict_first3[key] = expanded[key]\n",
    "\n",
    "run_large_query(dict_first3, \"headers_reindex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HEADERS Unique/clean comment Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0002 0004 0005 0006 0007 0008 0009 0010 0011 0016 0042 0077 0110 0115 0126 0157 0170 0174 0191 0195 0205 0209 0211 0221 0222 0226 0229 0233 0239 0240 0249 0253 0256 0261 0263 0265 0267 0271 0287 0297 0319 0331 0347 0360 0361 0362 0374 0376 0385 0399 0401 0404 0408 0419 0428 0437 0438 0467 0003_0 0003_1 0012_0 0012_1 0012_2 0012_3 0013_0 0013_1 0013_2 0013_3 0013_4 0013_5 0013_6 0013_7 0013_8 0013_9 0013_10 0013_11 0013_12 0014_0 0014_1 0081_0 0081_1 0138_0 0138_1 0138_2 0172_0 0172_1 0172_2 0172_3 0182_0 0182_1 0182_2 0189_0 0189_1 0190_0 0190_1 0190_2 0197_0 0197_1 0197_2 0197_3 0204_0 0204_1 0204_2 0204_3 0204_4 0204_5 0204_6 0204_7 0204_8 0207_0 0207_1 0208_0 0208_1 0208_2 0212_0 0212_1 0212_2 0212_3 0234_0 0234_1 0234_2 0241_0 0241_1 0241_2 0243_0 0243_1 0245_0 0245_1 0245_2 0245_3 0245_4 0245_5 0245_6 0245_7 0247_0 0247_1 0247_2 0247_3 0247_4 0247_5 0247_6 0254_0 0254_1 0254_2 0254_3 0254_4 0258_0 0258_1 0268_0 0268_1 0268_2 0301_0 0301_1 0301_2 0301_3 0301_4 0304_0 0304_1 0304_2 0304_3 0304_4 0308_0 0308_1 0308_2 0308_3 0308_4 0308_5 0308_6 0308_7 0308_8 0308_9 0308_10 0308_11 0308_12 0312_0 0312_1 0312_2 0312_3 0312_4 0322_0 0322_1 0322_2 0322_3 0322_4 0326_0 0326_1 0326_2 0326_3 0326_4 0326_5 0326_6 0326_7 0326_8 0326_9 0326_10 0336_0 0336_1 0346_0 0346_1 0346_2 0346_3 0346_4 0346_5 0346_6 0353_0 0353_1 0353_2 0353_3 0353_4 0353_5 0357_0 0357_1 0357_2 0357_3 0357_4 0357_5 0357_6 0357_7 0357_8 0357_9 0357_10 0383_0 0383_1 0386_0 0386_1 0386_2 0386_3 0386_4 0386_5 0386_6 0386_7 0386_8 0386_9 0386_10 0386_11 0386_12 0386_13 0386_14 0386_15 0386_16 0386_17 0386_18 0386_19 0386_20 0386_21 0386_22 0386_23 0386_24 0386_25 0386_26 0386_27 0423_0 0423_1 0423_2 0423_3 0423_4 0423_5 0423_6 "
     ]
    }
   ],
   "source": [
    "header_results = run_large_query(expanded, \"headers_standard\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to .json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump = json.dumps(header_results)\n",
    "f = open('match_header_results.json','w')\n",
    "f.write(dump)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HEADERS CUSTOM Unique/clean comment Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0002 0004 0005 0006 0007 0008 0009 0010 0011 0016 0042 0077 0110 0115 0126 0157 0170 0174 0191 0195 0205 0209 0211 0221 0222 0226 0229 0233 0239 0240 0249 0253 0256 0261 0263 0265 0267 0271 0287 0297 0319 0331 0347 0360 0361 0362 0374 0376 0385 0399 0401 0404 0408 0419 0428 0437 0438 0467 0003_0 0003_1 0012_0 0012_1 0012_2 0012_3 0013_0 0013_1 0013_2 0013_3 0013_4 0013_5 0013_6 0013_7 0013_8 0013_9 0013_10 0013_11 0013_12 0014_0 0014_1 0081_0 0081_1 0138_0 0138_1 0138_2 0172_0 0172_1 0172_2 0172_3 0182_0 0182_1 0182_2 0189_0 0189_1 0190_0 0190_1 0190_2 0197_0 0197_1 0197_2 0197_3 0204_0 0204_1 0204_2 0204_3 0204_4 0204_5 0204_6 0204_7 0204_8 0207_0 0207_1 0208_0 0208_1 0208_2 0212_0 0212_1 0212_2 0212_3 0234_0 0234_1 0234_2 0241_0 0241_1 0241_2 0243_0 0243_1 0245_0 0245_1 0245_2 0245_3 0245_4 0245_5 0245_6 0245_7 0247_0 0247_1 0247_2 0247_3 0247_4 0247_5 0247_6 0254_0 0254_1 0254_2 0254_3 0254_4 0258_0 0258_1 0268_0 0268_1 0268_2 0301_0 0301_1 0301_2 0301_3 0301_4 0304_0 0304_1 0304_2 0304_3 0304_4 0308_0 0308_1 0308_2 0308_3 0308_4 0308_5 0308_6 0308_7 0308_8 0308_9 0308_10 0308_11 0308_12 0312_0 0312_1 0312_2 0312_3 0312_4 0322_0 0322_1 0322_2 0322_3 0322_4 0326_0 0326_1 0326_2 0326_3 0326_4 0326_5 0326_6 0326_7 0326_8 0326_9 0326_10 0336_0 0336_1 0346_0 0346_1 0346_2 0346_3 0346_4 0346_5 0346_6 0353_0 0353_1 0353_2 0353_3 0353_4 0353_5 0357_0 0357_1 0357_2 0357_3 0357_4 0357_5 0357_6 0357_7 0357_8 0357_9 0357_10 0383_0 0383_1 0386_0 0386_1 0386_2 0386_3 0386_4 0386_5 0386_6 0386_7 0386_8 0386_9 0386_10 0386_11 0386_12 0386_13 0386_14 0386_15 0386_16 0386_17 0386_18 0386_19 0386_20 0386_21 0386_22 0386_23 0386_24 0386_25 0386_26 0386_27 0423_0 0423_1 0423_2 0423_3 0423_4 0423_5 0423_6 "
     ]
    }
   ],
   "source": [
    "header_custom_results = run_large_query(expanded, \"headers_custom\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to .json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump = json.dumps(header_custom_results)\n",
    "f = open('match_header_custom_results.json','w')\n",
    "f.write(dump)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HYBRID Unique/clean comment Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0002 0004 0005 0006 0007 0008 0009 0010 0011 0016 0042 0077 0110 0115 0126 0157 0170 0174 0191 0195 0205 0209 0211 0221 0222 0226 0229 0233 0239 0240 0249 0253 0256 0261 0263 0265 0267 0271 0287 0297 0319 0331 0347 0360 0361 0362 0374 0376 0385 0399 0401 0404 0408 0419 0428 0437 0438 0467 0003_0 0003_1 0012_0 0012_1 0012_2 0012_3 0013_0 0013_1 0013_2 0013_3 0013_4 0013_5 0013_6 0013_7 0013_8 0013_9 0013_10 0013_11 0013_12 0014_0 0014_1 0081_0 0081_1 0138_0 0138_1 0138_2 0172_0 0172_1 0172_2 0172_3 0182_0 0182_1 0182_2 0189_0 0189_1 0190_0 0190_1 0190_2 0197_0 0197_1 0197_2 0197_3 0204_0 0204_1 0204_2 0204_3 0204_4 0204_5 0204_6 0204_7 0204_8 0207_0 0207_1 0208_0 0208_1 0208_2 0212_0 0212_1 0212_2 0212_3 0234_0 0234_1 0234_2 0241_0 0241_1 0241_2 0243_0 0243_1 0245_0 0245_1 0245_2 0245_3 0245_4 0245_5 0245_6 0245_7 0247_0 0247_1 0247_2 0247_3 0247_4 0247_5 0247_6 0254_0 0254_1 0254_2 0254_3 0254_4 0258_0 0258_1 0268_0 0268_1 0268_2 0301_0 0301_1 0301_2 0301_3 0301_4 0304_0 0304_1 0304_2 0304_3 0304_4 0308_0 0308_1 0308_2 0308_3 0308_4 0308_5 0308_6 0308_7 0308_8 0308_9 0308_10 0308_11 0308_12 0312_0 0312_1 0312_2 0312_3 0312_4 0322_0 0322_1 0322_2 0322_3 0322_4 0326_0 0326_1 0326_2 0326_3 0326_4 0326_5 0326_6 0326_7 0326_8 0326_9 0326_10 0336_0 0336_1 0346_0 0346_1 0346_2 0346_3 0346_4 0346_5 0346_6 0353_0 0353_1 0353_2 0353_3 0353_4 0353_5 0357_0 0357_1 0357_2 0357_3 0357_4 0357_5 0357_6 0357_7 0357_8 0357_9 0357_10 0383_0 0383_1 0386_0 0386_1 0386_2 0386_3 0386_4 0386_5 0386_6 0386_7 0386_8 0386_9 0386_10 0386_11 0386_12 0386_13 0386_14 0386_15 0386_16 0386_17 0386_18 0386_19 0386_20 0386_21 0386_22 0386_23 0386_24 0386_25 0386_26 0386_27 0423_0 0423_1 0423_2 0423_3 0423_4 0423_5 0423_6 "
     ]
    }
   ],
   "source": [
    "hybrid_results = run_large_query(expanded, \"hybrid_standard\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to .json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump = json.dumps(hybrid_results)\n",
    "f = open('match_hybrid_results.json','w')\n",
    "f.write(dump)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HYBRID CUSTOM Unique/clean comment Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0002 0004 0005 0006 0007 0008 0009 0010 0011 0016 0042 0077 0110 0115 0126 0157 0170 0174 0191 0195 0205 0209 0211 0221 0222 0226 0229 0233 0239 0240 0249 0253 0256 0261 0263 0265 0267 0271 0287 0297 0319 0331 0347 0360 0361 0362 0374 0376 0385 0399 0401 0404 0408 0419 0428 0437 0438 0467 0003_0 0003_1 0012_0 0012_1 0012_2 0012_3 0013_0 0013_1 0013_2 0013_3 0013_4 0013_5 0013_6 0013_7 0013_8 0013_9 0013_10 0013_11 0013_12 0014_0 0014_1 0081_0 0081_1 0138_0 0138_1 0138_2 0172_0 0172_1 0172_2 0172_3 0182_0 0182_1 0182_2 0189_0 0189_1 0190_0 0190_1 0190_2 0197_0 0197_1 0197_2 0197_3 0204_0 0204_1 0204_2 0204_3 0204_4 0204_5 0204_6 0204_7 0204_8 0207_0 0207_1 0208_0 0208_1 0208_2 0212_0 0212_1 0212_2 0212_3 0234_0 0234_1 0234_2 0241_0 0241_1 0241_2 0243_0 0243_1 0245_0 0245_1 0245_2 0245_3 0245_4 0245_5 0245_6 0245_7 0247_0 0247_1 0247_2 0247_3 0247_4 0247_5 0247_6 0254_0 0254_1 0254_2 0254_3 0254_4 0258_0 0258_1 0268_0 0268_1 0268_2 0301_0 0301_1 0301_2 0301_3 0301_4 0304_0 0304_1 0304_2 0304_3 0304_4 0308_0 0308_1 0308_2 0308_3 0308_4 0308_5 0308_6 0308_7 0308_8 0308_9 0308_10 0308_11 0308_12 0312_0 0312_1 0312_2 0312_3 0312_4 0322_0 0322_1 0322_2 0322_3 0322_4 0326_0 0326_1 0326_2 0326_3 0326_4 0326_5 0326_6 0326_7 0326_8 0326_9 0326_10 0336_0 0336_1 0346_0 0346_1 0346_2 0346_3 0346_4 0346_5 0346_6 0353_0 0353_1 0353_2 0353_3 0353_4 0353_5 0357_0 0357_1 0357_2 0357_3 0357_4 0357_5 0357_6 0357_7 0357_8 0357_9 0357_10 0383_0 0383_1 0386_0 0386_1 0386_2 0386_3 0386_4 0386_5 0386_6 0386_7 0386_8 0386_9 0386_10 0386_11 0386_12 0386_13 0386_14 0386_15 0386_16 0386_17 0386_18 0386_19 0386_20 0386_21 0386_22 0386_23 0386_24 0386_25 0386_26 0386_27 0423_0 0423_1 0423_2 0423_3 0423_4 0423_5 0423_6 "
     ]
    }
   ],
   "source": [
    "hybrid_custom_results = run_large_query(expanded, \"hybrid_custom\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to .json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump = json.dumps(hybrid_custom_results)\n",
    "f = open('match_hybrid_custom_results.json','w')\n",
    "f.write(dump)\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
