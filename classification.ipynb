{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import string\n",
    "from sklearn.metrics import f1_score as f1, confusion_matrix as confusion, plot_roc_curve as roc\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, TweetTokenizer\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import xgboost as xgb\n",
    "from xgboost import plot_importance\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in Comments and True Classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"unique_comments2018.json\") as f:\n",
    "    texts = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = {key:value.replace(\"\\r\", \" \").replace(\"\\n\", \" \").replace(\"\\s\", \" \") for key, value in texts.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in texts.items():\n",
    "    texts[key] = ''.join(c for c in value if c in string.printable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_url = \"https://mikeanders.org/data/CMS/CMS-2018-0101-0001/CMS-1701-P%20Comment%20MetaData.csv\"\n",
    "data = pd.read_csv(metadata_url, usecols=range(0,36))[:468] #ignore last few columns and blank rows at end of csv \n",
    "data = data.rename(columns=lambda x: x.strip()) #strip whitespace from columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data10 = data.fillna(0) #fill NaN with 0\n",
    "section_cols = data10.columns[3:] \n",
    "data10[section_cols] = data10[section_cols].replace([\"Y\"], 1) #replace Y with 1 in approriate columns\n",
    "data11 = data10.copy()\n",
    "section_cols1  = data11.columns[3:] \n",
    "data11[section_cols1] = np.where((data11[section_cols1]  != 1),0,data11[section_cols1] )\n",
    "\n",
    "# Combining columns for index matching: (A6b, A6b.1, = A6b),  (C3b, C3b.1'= C3b) ('A7', 'A7.1', 'A7.2', = A7b, a7c),  (F = F2, F3)\n",
    "data11['A6b'] = (data11['A6b'] + data11['A6b.1'])\n",
    "data11['A6b'] = data11['A6b'].replace(2,1)\n",
    "data11['C3b'] = (data11['C3b'] + data11['C3b.1'])\n",
    "data11['C3b'] = data11['C3b'].replace(2,1)\n",
    "data11['A7'] = (data11['A7'] + data11['A7.1'] + data11['A7.2'])\n",
    "data11['A7'] = data11['A7'].replace(2,1)\n",
    "data11['A7'] = data11['A7'].replace(3,1)\n",
    "data11 = data11.drop(['A6b.1', 'C3b.1', 'A7.1', 'A7.2'], axis = 1)\n",
    "\n",
    "data11.Name = [name.split('DRAFT-')[1].split('-')[0] for name in data11.Name]\n",
    "data11 = data11.rename(columns=lambda x: x.lower())\n",
    "section_cols1 = data11.columns[3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data11 = data11.loc[data11['name'].isin(texts.keys())]\n",
    "data11[\"comment\"] = texts.values() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data11.sample(frac=.75, random_state=44)\n",
    "test = data11.drop(train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = list(train.comment)\n",
    "test_texts = list(test.comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>organization name / submitter name</th>\n",
       "      <th>submitter state</th>\n",
       "      <th>a2</th>\n",
       "      <th>a3</th>\n",
       "      <th>a4b</th>\n",
       "      <th>a4c</th>\n",
       "      <th>a5b</th>\n",
       "      <th>a5c</th>\n",
       "      <th>a5d</th>\n",
       "      <th>...</th>\n",
       "      <th>d3d</th>\n",
       "      <th>d4</th>\n",
       "      <th>e2</th>\n",
       "      <th>e3</th>\n",
       "      <th>e4</th>\n",
       "      <th>e5</th>\n",
       "      <th>e6</th>\n",
       "      <th>e7</th>\n",
       "      <th>f</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>0467</td>\n",
       "      <td>Washington State Hospital Association</td>\n",
       "      <td>WA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>see attached file(s)october 15, 2018 ms. seema...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>0408</td>\n",
       "      <td>Dana McCalley</td>\n",
       "      <td>FL</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>diabetic eye exam measure should be retired. t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>0207</td>\n",
       "      <td>Mayo Clinic</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>see attached file(s) mayo clinic 200 first str...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>0267</td>\n",
       "      <td>OneHealth Nebraska ACO, LLC</td>\n",
       "      <td>NE</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>see attached file(s) cms should modify the med...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0009</td>\n",
       "      <td>Sherman Jew</td>\n",
       "      <td>WI</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>making and enforcing more complex and expensiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>0374</td>\n",
       "      <td>The Queen's Health System</td>\n",
       "      <td>HI</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>please see attached for comments. ms. seema ve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>0195</td>\n",
       "      <td>Michael Saito</td>\n",
       "      <td>WI</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>please see the attached document with epic's c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>0347</td>\n",
       "      <td>American Association of Nurse Practitioners</td>\n",
       "      <td>VA</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>please find the attached comments of the ameri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>0249</td>\n",
       "      <td>High Value Healcare Collaborative</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>see attached file(s) high value healthcare col...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>0253</td>\n",
       "      <td>Innova Health System</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>see attached file(s)   signature  october 15, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     name           organization name / submitter name submitter state  a2  \\\n",
       "465  0467        Washington State Hospital Association              WA   0   \n",
       "406  0408                                Dana McCalley              FL   0   \n",
       "205  0207                                  Mayo Clinic               0   0   \n",
       "265  0267                  OneHealth Nebraska ACO, LLC              NE   1   \n",
       "7    0009                                  Sherman Jew              WI   0   \n",
       "..    ...                                          ...             ...  ..   \n",
       "372  0374                    The Queen's Health System              HI   0   \n",
       "193  0195                                Michael Saito              WI   0   \n",
       "345  0347  American Association of Nurse Practitioners              VA   1   \n",
       "247  0249            High Value Healcare Collaborative               0   0   \n",
       "251  0253                         Innova Health System               0   0   \n",
       "\n",
       "     a3  a4b  a4c  a5b  a5c  a5d  ...  d3d  d4  e2  e3  e4  e5  e6  e7  f  \\\n",
       "465   0    0    0    0    0    0  ...    0   0   0   0   0   0   0   0  0   \n",
       "406   0    0    0    0    0    0  ...    0   0   0   0   0   0   0   0  0   \n",
       "205   1    0    1    0    0    0  ...    0   0   1   1   0   0   0   0  0   \n",
       "265   1    0    1    1    0    0  ...    0   0   0   0   0   0   0   0  0   \n",
       "7     0    0    0    1    0    0  ...    0   0   0   0   0   0   0   0  0   \n",
       "..   ..  ...  ...  ...  ...  ...  ...  ...  ..  ..  ..  ..  ..  ..  .. ..   \n",
       "372   1    0    0    1    0    0  ...    0   0   0   0   0   0   0   0  0   \n",
       "193   0    0    0    0    0    0  ...    0   0   0   0   0   1   0   0  0   \n",
       "345   1    0    0    0    0    0  ...    0   0   1   0   0   0   0   0  0   \n",
       "247   1    0    1    0    0    0  ...    0   0   0   0   0   0   0   0  0   \n",
       "251   1    0    0    1    0    0  ...    0   0   0   0   0   0   1   0  0   \n",
       "\n",
       "                                               comment  \n",
       "465  see attached file(s)october 15, 2018 ms. seema...  \n",
       "406  diabetic eye exam measure should be retired. t...  \n",
       "205  see attached file(s) mayo clinic 200 first str...  \n",
       "265  see attached file(s) cms should modify the med...  \n",
       "7    making and enforcing more complex and expensiv...  \n",
       "..                                                 ...  \n",
       "372  please see attached for comments. ms. seema ve...  \n",
       "193  please see the attached document with epic's c...  \n",
       "345  please find the attached comments of the ameri...  \n",
       "247  see attached file(s) high value healthcare col...  \n",
       "251  see attached file(s)   signature  october 15, ...  \n",
       "\n",
       "[70 rows x 33 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a2      32\n",
       "a3      46\n",
       "a4b      4\n",
       "a4c     29\n",
       "a5b     32\n",
       "a5c     17\n",
       "a5d     16\n",
       "a6b     12\n",
       "a6c     10\n",
       "a6d2     2\n",
       "a6d3     7\n",
       "a7      24\n",
       "b2a     27\n",
       "b2b     24\n",
       "c2      27\n",
       "c3a     20\n",
       "c3b     17\n",
       "d2      39\n",
       "d3b     26\n",
       "d3c     16\n",
       "d3d      3\n",
       "d4       1\n",
       "e2      16\n",
       "e3      10\n",
       "e4       5\n",
       "e5      25\n",
       "e6      20\n",
       "e7       9\n",
       "f        2\n",
       "dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data11.sum(axis=0)[section_cols1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify One Rule Section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['!', '\"', \"#\", \"$\", \"%\", \"&\", \"(\", \")\", \"*\", \"+\", \",\", \"-\", \".\", \"/\", \":\", \";\", \"<\", \"=\", \">\", \"?\", \"@\", \"[\", \"]\", \"^\", \"_\", \"`\", \"{\", \"|\", \"}\", \"~\", \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOW Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vector = CountVectorizer(tokenizer=nltk.RegexpTokenizer(r\"\\w+\").tokenize, ngram_range=(1,2), stop_words='english')\n",
    "\n",
    "x_train = bow_vector.fit_transform(train_texts)\n",
    "y_train = np.array(train.a2)\n",
    "\n",
    "x_test = bow_vector.transform(test_texts)\n",
    "y_test = np.array(test.a2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       1, 0], dtype=int64)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm = SGDClassifier(random_state=44)\n",
    "svm.fit(X=x_train, y=y_train)\n",
    "svm_preds = svm.predict(x_test)\n",
    "svm_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4444444444444444"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_f1 = f1(y_test, svm_preds)\n",
    "svm_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10,  2],\n",
       "       [ 8,  4]], dtype=int64)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_confusion = confusion(y_test, svm_preds)\n",
    "svm_confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identifying Most Significant Words for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['texas',\n",
       " 'cms',\n",
       " 'amga',\n",
       " 'e',\n",
       " 'agency',\n",
       " 'rural',\n",
       " 'acos',\n",
       " 'program',\n",
       " 's',\n",
       " 'percent',\n",
       " 'financial',\n",
       " 'aco',\n",
       " 'aco s',\n",
       " 'savings',\n",
       " 'quality']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefs = np.abs(svm.coef_[0])\n",
    "top_fifteen = np.argpartition(coefs, -15)[-15:]\n",
    "[(bow_vector.get_feature_names()[feature]) for feature in top_fifteen] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "boost = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "boost.fit(x_train, y_train)\n",
    "boost_preds = boost.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4444444444444444"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boost_f1 = f1(y_test, svm_preds)\n",
    "boost_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10,  2],\n",
       "       [10,  2]], dtype=int64)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boost_confusion = confusion(y_test, boost_preds)\n",
    "boost_confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identifying Most Significant Words for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['waivers',\n",
       " 'successful',\n",
       " 'cause',\n",
       " 'p',\n",
       " 'capital',\n",
       " 'choose',\n",
       " 'administrator',\n",
       " 'verma',\n",
       " 'behalf',\n",
       " '1',\n",
       " 'assignment',\n",
       " 'coordination',\n",
       " 'hospital',\n",
       " 'medicare shared',\n",
       " 'medicare']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boost_top15 = np.argsort(-boost.feature_importances_)[0:15]\n",
    "[(bow_vector.get_feature_names()[feature]) for feature in boost_top15] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify All Rule Sections - BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vector = CountVectorizer(tokenizer=nltk.RegexpTokenizer(r\"\\w+\").tokenize, ngram_range=(1,2), stop_words=\"english\")\n",
    "\n",
    "x_train = bow_vector.fit_transform(train_texts)\n",
    "y_train = np.array(train[section_cols1])\n",
    "\n",
    "x_test = bow_vector.transform(test_texts)\n",
    "y_test = np.array(test[section_cols1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,\n",
       "        0, 0, 0, 1, 1, 1, 0],\n",
       "       [1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,\n",
       "        0, 0, 0, 1, 1, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,\n",
       "        0, 0, 0, 1, 1, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,\n",
       "        1, 0, 0, 1, 0, 0, 0]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boost = xgb.XGBClassifier(objective='binary:logistic', use_label_encoder=False, eval_metric='logloss')\n",
    "\n",
    "multi_boost = MultiOutputClassifier(boost)\n",
    "\n",
    "multi_boost.fit(x_train, y_train)\n",
    "\n",
    "multi_boost_preds = multi_boost.predict(x_test)\n",
    "\n",
    "multi_boost_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_boost_f1 = f1(y_test, multi_boost_preds, zero_division=0, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a2': 0.25,\n",
       " 'a3': 0.75,\n",
       " 'a4b': 0.0,\n",
       " 'a4c': 0.5,\n",
       " 'a5b': 0.7777777777777777,\n",
       " 'a5c': 0.4,\n",
       " 'a5d': 0.0,\n",
       " 'a6b': 0.4,\n",
       " 'a6c': 0.0,\n",
       " 'a6d2': 0.0,\n",
       " 'a6d3': 0.0,\n",
       " 'a7': 0.6153846153846154,\n",
       " 'b2a': 0.6250000000000001,\n",
       " 'b2b': 0.8333333333333333,\n",
       " 'c2': 0.42857142857142855,\n",
       " 'c3a': 0.2222222222222222,\n",
       " 'c3b': 0.6,\n",
       " 'd2': 0.5454545454545454,\n",
       " 'd3b': 0.7142857142857143,\n",
       " 'd3c': 0.3636363636363636,\n",
       " 'd3d': 0.0,\n",
       " 'd4': 0.0,\n",
       " 'e2': 0.0,\n",
       " 'e3': 0.0,\n",
       " 'e4': 0.0,\n",
       " 'e5': 0.8333333333333334,\n",
       " 'e6': 0.6666666666666666,\n",
       " 'e7': 0.6666666666666666,\n",
       " 'f': 0.0}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = {sec:score for (sec, score) in zip(section_cols1, list(multi_boost_f1))}\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35145974714940226"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(multi_boost_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identifying Most Significant Words for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:749: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return all_features / all_features.sum()\n"
     ]
    }
   ],
   "source": [
    "features = []\n",
    "for boost in multi_boost.estimators_:\n",
    "    boost_top10 = np.argsort(-boost.feature_importances_)[0:10]\n",
    "    features.append([(bow_vector.get_feature_names()[feature]) for feature in boost_top10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a2': ['waivers',\n",
       "  'successful',\n",
       "  'cause',\n",
       "  'p',\n",
       "  'capital',\n",
       "  'choose',\n",
       "  'administrator',\n",
       "  'verma',\n",
       "  'behalf',\n",
       "  '1'],\n",
       " 'a4b': ['build high',\n",
       "  'benefit enhancements',\n",
       "  'cms1701p',\n",
       "  'incentive program',\n",
       "  'years upside',\n",
       "  'operation',\n",
       "  'repayment',\n",
       "  'patient care',\n",
       "  'reward',\n",
       "  'based care'],\n",
       " 'a5c': ['agreement period',\n",
       "  'oppose',\n",
       "  'participation options',\n",
       "  'determining',\n",
       "  'comments',\n",
       "  'regardless risk',\n",
       "  'believe acos',\n",
       "  'reduction',\n",
       "  'higher risk',\n",
       "  'regardless'],\n",
       " 'a5d': ['basic',\n",
       "  'losses',\n",
       "  'acos like',\n",
       "  'telehealth',\n",
       "  'agree',\n",
       "  'cut',\n",
       "  '10',\n",
       "  'participate',\n",
       "  'greater',\n",
       "  'achieve'],\n",
       " 'a6b': ['msr mlr',\n",
       "  'cms provide',\n",
       "  'time',\n",
       "  'date',\n",
       "  'tracks',\n",
       "  '0',\n",
       "  'pend oreille',\n",
       "  'pending',\n",
       "  'pending medicare',\n",
       "  'pending physician'],\n",
       " 'a6c': ['funds',\n",
       "  'simply',\n",
       "  'attractive',\n",
       "  'mechanism',\n",
       "  'sided',\n",
       "  'administrator',\n",
       "  'program',\n",
       "  'greater',\n",
       "  'services',\n",
       "  'medicare'],\n",
       " 'a6d2': ['0',\n",
       "  'penalty spending',\n",
       "  'penalty sub',\n",
       "  'pend',\n",
       "  'pend oreille',\n",
       "  'pending',\n",
       "  'pending medicare',\n",
       "  'pending physician',\n",
       "  'penetration',\n",
       "  'penetration large'],\n",
       " 'a6d3': ['information make',\n",
       "  'benefit enhancements',\n",
       "  'early termination',\n",
       "  'msr mlr',\n",
       "  'election',\n",
       "  'termination',\n",
       "  'june',\n",
       "  'results',\n",
       "  'proposal',\n",
       "  'patient'],\n",
       " 'c2': ['beneficiary incentives',\n",
       "  'variable',\n",
       "  'allow',\n",
       "  'advantage',\n",
       "  '12',\n",
       "  'community',\n",
       "  'verma',\n",
       "  'appropriately',\n",
       "  'agreement periods',\n",
       "  'cause'],\n",
       " 'c3a': ['benchmarking',\n",
       "  'beneficiary assignment',\n",
       "  'urges',\n",
       "  'risk adjustment',\n",
       "  'investments',\n",
       "  'identify',\n",
       "  'opt',\n",
       "  'renewing',\n",
       "  'experience',\n",
       "  'language'],\n",
       " 'd3c': ['incentives acos',\n",
       "  'consideration',\n",
       "  'risk acos',\n",
       "  'defining',\n",
       "  'center',\n",
       "  'community',\n",
       "  'department health',\n",
       "  '1 2019',\n",
       "  'historical',\n",
       "  'regional'],\n",
       " 'd3d': ['45',\n",
       "  'accurate',\n",
       "  'cases',\n",
       "  'benchmarking',\n",
       "  '26',\n",
       "  'day',\n",
       "  'better',\n",
       "  'models',\n",
       "  '0',\n",
       "  '1'],\n",
       " 'd4': ['0',\n",
       "  'penalty spending',\n",
       "  'penalty sub',\n",
       "  'pend',\n",
       "  'pend oreille',\n",
       "  'pending',\n",
       "  'pending medicare',\n",
       "  'pending physician',\n",
       "  'penetration',\n",
       "  'penetration large'],\n",
       " 'e2': ['opt',\n",
       "  'voluntary',\n",
       "  'methods',\n",
       "  'alignment',\n",
       "  'patient',\n",
       "  'needs',\n",
       "  'retrospective',\n",
       "  'following',\n",
       "  'list',\n",
       "  'visits'],\n",
       " 'e3': ['beginning',\n",
       "  'inexperienced',\n",
       "  'codes',\n",
       "  '2018 ms',\n",
       "  'beneficiaries',\n",
       "  'real time',\n",
       "  'ask',\n",
       "  'path',\n",
       "  'medical',\n",
       "  'currently'],\n",
       " 'e4': ['1',\n",
       "  '0',\n",
       "  '21',\n",
       "  '31',\n",
       "  'aco professional',\n",
       "  'january',\n",
       "  '5 years',\n",
       "  'just',\n",
       "  'add',\n",
       "  'technology'],\n",
       " 'f': ['clarification',\n",
       "  'track',\n",
       "  'track 1',\n",
       "  'current',\n",
       "  'health',\n",
       "  'risk',\n",
       "  'medicare',\n",
       "  'pending medicare',\n",
       "  'pending physician',\n",
       "  'penetration']}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "important_features = {sec:feature_list for (sec, feature_list) in zip(section_cols1, features)}\n",
    "{key:features for (key, value), (key1, features) in zip(scores.items(), important_features.items()) if value < 0.5}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify All Rule Sections - TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vector = TfidfVectorizer(tokenizer=nltk.RegexpTokenizer(r\"\\w+\").tokenize, ngram_range=(1,2), stop_words=\"english\")\n",
    "\n",
    "x_train = tfidf_vector.fit_transform(train_texts)\n",
    "y_train = np.array(train[section_cols1])\n",
    "\n",
    "x_test = tfidf_vector.transform(test_texts)\n",
    "y_test = np.array(test[section_cols1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
       "        0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 1, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,\n",
       "        1, 0, 0, 1, 1, 0, 0],\n",
       "       [1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,\n",
       "        0, 1, 0, 1, 1, 0, 0],\n",
       "       [1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,\n",
       "        1, 1, 0, 1, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,\n",
       "        0, 0, 0, 1, 1, 0, 0]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boost = xgb.XGBClassifier(objective='binary:logistic', use_label_encoder=False, eval_metric='logloss')\n",
    "\n",
    "multi_boost = MultiOutputClassifier(boost)\n",
    "\n",
    "multi_boost.fit(x_train, y_train)\n",
    "\n",
    "multi_boost_preds = multi_boost.predict(x_test)\n",
    "\n",
    "multi_boost_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_boost_f1 = f1(y_test, multi_boost_preds, zero_division=0, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a2': 0.6666666666666666,\n",
       " 'a3': 0.7200000000000001,\n",
       " 'a4b': 0.0,\n",
       " 'a4c': 0.75,\n",
       " 'a5b': 0.8000000000000002,\n",
       " 'a5c': 0.6,\n",
       " 'a5d': 0.25,\n",
       " 'a6b': 0.6666666666666666,\n",
       " 'a6c': 0.0,\n",
       " 'a6d2': 0.0,\n",
       " 'a6d3': 0.0,\n",
       " 'a7': 0.5,\n",
       " 'b2a': 0.5333333333333333,\n",
       " 'b2b': 0.7692307692307692,\n",
       " 'c2': 0.6153846153846154,\n",
       " 'c3a': 0.2222222222222222,\n",
       " 'c3b': 0.5,\n",
       " 'd2': 0.7857142857142857,\n",
       " 'd3b': 0.75,\n",
       " 'd3c': 0.19999999999999998,\n",
       " 'd3d': 0.0,\n",
       " 'd4': 0.0,\n",
       " 'e2': 0.5,\n",
       " 'e3': 0.6666666666666666,\n",
       " 'e4': 0.0,\n",
       " 'e5': 0.7142857142857143,\n",
       " 'e6': 0.7692307692307693,\n",
       " 'e7': 0.0,\n",
       " 'f': 0.0}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = {sec:score for (sec, score) in zip(section_cols1, list(multi_boost_f1))}\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4130828175655762"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(multi_boost_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identifying Most Significant Words for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:749: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return all_features / all_features.sum()\n"
     ]
    }
   ],
   "source": [
    "features = []\n",
    "for boost in multi_boost.estimators_:\n",
    "    boost_top10 = np.argsort(-boost.feature_importances_)[0:10]\n",
    "    features.append([(bow_vector.get_feature_names()[feature]) for feature in boost_top10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a4b': ['beneficiary incentive',\n",
       "  'avoid',\n",
       "  'beneficiary',\n",
       "  'cms1701p',\n",
       "  'enhancements',\n",
       "  'benefit enhancements',\n",
       "  'improve care',\n",
       "  'patient',\n",
       "  'build',\n",
       "  'new'],\n",
       " 'a5d': ['telehealth',\n",
       "  'losses',\n",
       "  'infrastructure',\n",
       "  'agree',\n",
       "  'community',\n",
       "  'recommend',\n",
       "  'greater',\n",
       "  'performance',\n",
       "  'based',\n",
       "  'cms'],\n",
       " 'a6c': ['funds',\n",
       "  'administrator',\n",
       "  'mechanism',\n",
       "  'attractive',\n",
       "  '1701',\n",
       "  'shared',\n",
       "  'healthcare',\n",
       "  'o',\n",
       "  'mssp',\n",
       "  'sided'],\n",
       " 'a6d2': ['0',\n",
       "  'penalty spending',\n",
       "  'penalty sub',\n",
       "  'pend',\n",
       "  'pend oreille',\n",
       "  'pending',\n",
       "  'pending medicare',\n",
       "  'pending physician',\n",
       "  'penetration',\n",
       "  'penetration large'],\n",
       " 'a6d3': ['information make',\n",
       "  'benefit enhancements',\n",
       "  'mlr acos',\n",
       "  'insufficient',\n",
       "  'possible',\n",
       "  'beneficiary incentive',\n",
       "  '4',\n",
       "  'instead',\n",
       "  'long',\n",
       "  'program accountable'],\n",
       " 'c3a': ['beneficiary assignment',\n",
       "  'chief',\n",
       "  'percent',\n",
       "  'urges',\n",
       "  'implementation',\n",
       "  '2018 ms',\n",
       "  'costs',\n",
       "  'investments',\n",
       "  'submit',\n",
       "  'earlier'],\n",
       " 'd3c': ['acos provide',\n",
       "  'approximately',\n",
       "  'risk acos',\n",
       "  'defining',\n",
       "  'incentives',\n",
       "  'hcc',\n",
       "  'new',\n",
       "  'center',\n",
       "  'community',\n",
       "  'adjustment'],\n",
       " 'd3d': ['8',\n",
       "  '1 2',\n",
       "  '6',\n",
       "  'm',\n",
       "  'models',\n",
       "  'believe',\n",
       "  'practices',\n",
       "  'model',\n",
       "  '0',\n",
       "  'reduce'],\n",
       " 'd4': ['0',\n",
       "  'penalty spending',\n",
       "  'penalty sub',\n",
       "  'pend',\n",
       "  'pend oreille',\n",
       "  'pending',\n",
       "  'pending medicare',\n",
       "  'pending physician',\n",
       "  'penetration',\n",
       "  'penetration large'],\n",
       " 'e4': ['0',\n",
       "  '000',\n",
       "  '1 2019',\n",
       "  'agreement',\n",
       "  '1 acos',\n",
       "  'acos performance',\n",
       "  'align',\n",
       "  'care coordination',\n",
       "  'states',\n",
       "  'current'],\n",
       " 'e7': ['pharmacy',\n",
       "  'healthcare costs',\n",
       "  'aligned',\n",
       "  'mips',\n",
       "  's',\n",
       "  'include',\n",
       "  'sets',\n",
       "  'department',\n",
       "  'use',\n",
       "  'encourage'],\n",
       " 'f': ['clarification',\n",
       "  'understanding',\n",
       "  'currently',\n",
       "  'current',\n",
       "  'track 1',\n",
       "  'risk',\n",
       "  'based',\n",
       "  'medicare',\n",
       "  'pending medicare',\n",
       "  'pending physician']}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "important_features = {sec:feature_list for (sec, feature_list) in zip(section_cols1, features)}\n",
    "{key:features for (key, value), (key1, features) in zip(scores.items(), important_features.items()) if value < 0.5}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
