{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import math\n",
    "from statistics import mode\n",
    "from sklearn.metrics import ndcg_score, f1_score, average_precision_score\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_url = \"https://mikeanders.org/data/CMS/CMS-2018-0101-0001/CMS-1701-P%20Comment%20MetaData.csv\"\n",
    "data = pd.read_csv(metadata_url, usecols=range(0,36))[:469] #ignore last few columns and blank rows at end of csv \n",
    "data = data.rename(columns=lambda x: x.strip()) #strip whitespace from columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "data10 = data.fillna(0) #fill NaN with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "section_cols = data10.columns[3:] \n",
    "data10[section_cols] = data10[section_cols].replace([\"Y\"], 1) #replace Y with 1 in approriate columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['A2', 'A3', 'A4b', 'A4c', 'A5b', 'A5c', 'A5d', 'A6b', 'A6b.1', 'A6c',\n",
       "       'A6d2', 'A6d3', 'A7', 'A7.1', 'A7.2', 'B2a', 'B2b', 'C2', 'C3a', 'C3b',\n",
       "       'C3b.1', 'D2', 'D3b', 'D3c', 'D3d', 'D4', 'E2', 'E3', 'E4', 'E5', 'E6',\n",
       "       'E7', 'F'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "section_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "data11 = data10.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "section_cols1  = data11.columns[3:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Organization Name / Submitter name</th>\n",
       "      <th>Submitter State</th>\n",
       "      <th>A2</th>\n",
       "      <th>A3</th>\n",
       "      <th>A4b</th>\n",
       "      <th>A4c</th>\n",
       "      <th>A5b</th>\n",
       "      <th>A5c</th>\n",
       "      <th>A5d</th>\n",
       "      <th>...</th>\n",
       "      <th>D3c</th>\n",
       "      <th>D3d</th>\n",
       "      <th>D4</th>\n",
       "      <th>E2</th>\n",
       "      <th>E3</th>\n",
       "      <th>E4</th>\n",
       "      <th>E5</th>\n",
       "      <th>E6</th>\n",
       "      <th>E7</th>\n",
       "      <th>F</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CMS-2018-0101-DRAFT-0002-08_23_2018-05_17_PM</td>\n",
       "      <td>Erick Meleher</td>\n",
       "      <td>NC</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CMS-2018-0101-DRAFT-0003-08_23_2018-05_18_PM</td>\n",
       "      <td>Mayank Shah</td>\n",
       "      <td>IL</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CMS-2018-0101-DRAFT-0004-08_24_2018-01_16_PM</td>\n",
       "      <td>Mayank Shah</td>\n",
       "      <td>IL</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CMS-2018-0101-DRAFT-0005-08_29_2018-10_05_AM</td>\n",
       "      <td>Morey Menacker</td>\n",
       "      <td>NJ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CMS-2018-0101-DRAFT-0006-08_30_2018-09_14_AM</td>\n",
       "      <td>Todd Rapoza</td>\n",
       "      <td>MA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td>CMS-2018-0101-DRAFT-0466-10_17_2018-04_53_AM</td>\n",
       "      <td>Liberty ACO</td>\n",
       "      <td>TX</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>CMS-2018-0101-DRAFT-0467-10_17_2018-04_53_AM</td>\n",
       "      <td>Washington State Hospital Association</td>\n",
       "      <td>WA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>CMS-2018-0101-DRAFT-0468-10_17_2018-05_02_AM</td>\n",
       "      <td>Think Whole Person Healthcare</td>\n",
       "      <td>NE</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>CMS-2018-0101-DRAFT-0469-10_17_2018-05_02_AM</td>\n",
       "      <td>Palm Beach Accountable Care Organization</td>\n",
       "      <td>FL</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>CMS-2018-0101-DRAFT-0470-10_17_2018-05_02_AM</td>\n",
       "      <td>Central Florida ACO</td>\n",
       "      <td>FL</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>469 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Name  \\\n",
       "0    CMS-2018-0101-DRAFT-0002-08_23_2018-05_17_PM   \n",
       "1    CMS-2018-0101-DRAFT-0003-08_23_2018-05_18_PM   \n",
       "2    CMS-2018-0101-DRAFT-0004-08_24_2018-01_16_PM   \n",
       "3    CMS-2018-0101-DRAFT-0005-08_29_2018-10_05_AM   \n",
       "4    CMS-2018-0101-DRAFT-0006-08_30_2018-09_14_AM   \n",
       "..                                            ...   \n",
       "464  CMS-2018-0101-DRAFT-0466-10_17_2018-04_53_AM   \n",
       "465  CMS-2018-0101-DRAFT-0467-10_17_2018-04_53_AM   \n",
       "466  CMS-2018-0101-DRAFT-0468-10_17_2018-05_02_AM   \n",
       "467  CMS-2018-0101-DRAFT-0469-10_17_2018-05_02_AM   \n",
       "468  CMS-2018-0101-DRAFT-0470-10_17_2018-05_02_AM   \n",
       "\n",
       "           Organization Name / Submitter name Submitter State  A2  A3  A4b  \\\n",
       "0                               Erick Meleher              NC   0   0    0   \n",
       "1                                 Mayank Shah              IL   0   0    0   \n",
       "2                                 Mayank Shah              IL   0   0    0   \n",
       "3                              Morey Menacker              NJ   0   0    0   \n",
       "4                                 Todd Rapoza              MA   0   0    0   \n",
       "..                                        ...             ...  ..  ..  ...   \n",
       "464                               Liberty ACO              TX   0   1    0   \n",
       "465     Washington State Hospital Association              WA   0   0    0   \n",
       "466             Think Whole Person Healthcare              NE   1   0    0   \n",
       "467  Palm Beach Accountable Care Organization              FL   0   1    0   \n",
       "468                       Central Florida ACO              FL   0   1    0   \n",
       "\n",
       "     A4c  A5b  A5c  A5d  ...  D3c  D3d  D4  E2  E3  E4  E5  E6  E7  F  \n",
       "0      0    0    0    0  ...    0    0   0   0   0   0   0   0   0  0  \n",
       "1      0    0    0    0  ...    1    0   0   0   0   0   1   0   0  0  \n",
       "2      0    0    0    0  ...    0    0   0   0   0   0   0   0   0  0  \n",
       "3      0    1    0    0  ...    0    0   0   0   0   0   0   0   0  0  \n",
       "4      0    0    0    0  ...    0    0   0   0   0   0   0   0   0  1  \n",
       "..   ...  ...  ...  ...  ...  ...  ...  ..  ..  ..  ..  ..  ..  .. ..  \n",
       "464    0    0    0    0  ...    0    0   0   0   0   0   0   0   0  0  \n",
       "465    0    0    0    0  ...    0    0   0   0   0   0   0   0   0  0  \n",
       "466    0    0    0    0  ...    0    0   0   0   0   0   0   0   0  0  \n",
       "467    1    1    1    0  ...    1    0   0   0   1   1   0   1   0  0  \n",
       "468    0    0    0    0  ...    0    0   0   0   0   0   0   0   0  0  \n",
       "\n",
       "[469 rows x 36 columns]"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data11[section_cols1] = np.where((data11[section_cols1]  != 1),0,data11[section_cols1] )\n",
    "data11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    448\n",
       "1     21\n",
       "Name: A6b, dtype: int64"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combining columns for index matching: (A6b, A6b.1, = A6b),  (C3b, C3b.1'= C3b) ('A7', 'A7.1', 'A7.2', = A7b, a7c),  (F = F2, F3)\n",
    "\n",
    "data11['A6b'] = (data11['A6b'] + data11['A6b.1'])\n",
    "data11['A6b'] = data11['A6b'].replace(2,1)\n",
    "data11['A6b'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    413\n",
       "1     56\n",
       "Name: C3b, dtype: int64"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data11['C3b'] = (data11['C3b'] + data11['C3b.1'])\n",
    "data11['C3b'] = data11['C3b'].replace(2,1)\n",
    "data11['C3b'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    418\n",
       "1     51\n",
       "Name: A7, dtype: int64"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data11['A7'] = (data11['A7'] + data11['A7.1'] + data11['A7.2'])\n",
    "data11['A7'] = data11['A7'].replace(2,1)\n",
    "data11['A7'] = data11['A7'].replace(3,1)\n",
    "data11['A7'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "data11 = data11.drop(['A6b.1', 'C3b.1', 'A7.1', 'A7.2'], axis = 1)\n",
    "data11 = data11[0:468]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['A2', 'A3', 'A4b', 'A4c', 'A5b', 'A5c', 'A5d', 'A6b', 'A6c', 'A6d2',\n",
       "       'A6d3', 'A7', 'B2a', 'B2b', 'C2', 'C3a', 'C3b', 'D2', 'D3b', 'D3c',\n",
       "       'D3d', 'D4', 'E2', 'E3', 'E4', 'E5', 'E6', 'E7', 'F'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "section_cols1  = data11.columns[3:]\n",
    "section_cols1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth_dict = {} # initiate dict\n",
    "for i, name in enumerate(data11.Name): # iterate through Comment Names\n",
    "    dict1 = {col:data11[col].iloc[i] for col in section_cols1} # locate this row's values for each column\n",
    "    list1 = [key.lower() for key,value in dict1.items() if value==1] # create list of matching sections for this row\n",
    "    name1 = str(name).split('DRAFT-')[1].split('-')[0] # format Comment Name\n",
    "    truth_dict[name1] = list1 # add Comment Name and list of matching sections to dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump = json.dumps(truth_dict)\n",
    "f = open('truth_dict.json','w')\n",
    "f.write(dump)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_matches = {key: len(value) for key, value in truth_dict.items()}\n",
    "mode(list(num_matches.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define true rule section names\n",
    "truth_keys = list(section_cols1)\n",
    "truth_keys = [key1.lower() for key1 in truth_keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_ones = {'0083': 'good luck everyone!',\n",
    " '0142': 'see attached file(s)',\n",
    " '0237': 'see attached file(s)\\n',\n",
    " '0270': 'see attached file(s)\\n',\n",
    " '0293': 'see attached file(s)',\n",
    " '0321': 'see attached file(s)\\n\\n',\n",
    " '0324': 'see attachement\\n',\n",
    " '0329': 'see attached file(s)\\n',\n",
    " '0342': 'see attached file(s)\\n',\n",
    " '0447': 'see attached file(s)'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{key: truth_dict[key] for key in list(short_ones.keys())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#{key: value for key, value in truth_dict.items() if not value}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Header Index Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('header_results.json') as f:\n",
    "    header_results_json = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tranlate ES keys to true rule section names, MAINTAINING ORDER\n",
    "for key, value in header_results_json.items():\n",
    "    value_list = []\n",
    "    for val_idx in value:\n",
    "        for true_key in truth_keys:\n",
    "            if true_key in val_idx and true_key not in value_list: \n",
    "                value_list.append(true_key)\n",
    "        header_results_json[key]=value_list     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match_dict shows all true positives, without penalty to false categorization\n",
    "match_dict= {}\n",
    "for (key1, value1), (key2,value2) in zip(truth_dict.items(), header_results_json.items()):\n",
    "    match_count = 0\n",
    "    for val_idx in value1:\n",
    "        if val_idx in value2:\n",
    "            match_count += 1\n",
    "        match_dict[key1]=match_count/len(value1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpenalized accuracy of true positives\n",
    "meanvals = np.mean(list(match_dict.values()))\n",
    "meanvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match_dict1 shows all true positives, with penalty to false categorization\n",
    "match_dict1= {}\n",
    "for (key1, value1), (key2,value2) in zip(truth_dict.items(), header_results_json.items()):\n",
    "    match_count = 0\n",
    "    for val_idx in value2:\n",
    "        if val_idx in value1:\n",
    "            match_count += 1\n",
    "        match_dict1[key1]=match_count/len(value2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# penalized accuracy of true positives\n",
    "meanvals1 = np.mean(list(match_dict1.values()))\n",
    "meanvals1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([meanvals,meanvals1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sklearn NDCG Headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create truth dict with values equal to sparse array length=29, binary\n",
    "truth_dict_all = truth_dict.copy()\n",
    "for key, value in truth_dict.items():\n",
    "    new_value_list = np.zeros(len(truth_keys))\n",
    "    for val in value:\n",
    "        for i in range(len(truth_keys)):\n",
    "            if val == truth_keys[i]:\n",
    "                new_value_list[i] = 1\n",
    "    truth_dict_all[key] = new_value_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create header results dict with values equal to sparse array length=29, binary\n",
    "binary_header_results_all = header_results_json.copy()\n",
    "for key, value in header_results_json.items():\n",
    "    new_value_list = np.zeros(len(truth_keys))\n",
    "    counter = 1\n",
    "    for val in value:\n",
    "        for i in range(len(truth_keys)):\n",
    "            if val == truth_keys[i]:\n",
    "                counter += 1\n",
    "                new_value_list[i] = 1\n",
    "    binary_header_results_all[key] = new_value_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create header results dict with values equal to sparse array length=29, (discounted)\n",
    "header_results_all = header_results_json.copy()\n",
    "for key, value in header_results_json.items():\n",
    "    new_value_list = np.zeros(len(truth_keys))\n",
    "    counter = 1\n",
    "    for val in value:\n",
    "        for i in range(len(truth_keys)):\n",
    "            if val == truth_keys[i]:\n",
    "                counter += 1\n",
    "                new_value_list[i] = 1/math.log(counter, 2)\n",
    "    header_results_all[key] = new_value_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers_ndcg = {}\n",
    "headers_ap = {}\n",
    "headers_f1 = {}\n",
    "for (true_key, true_value), (es_key, es_value) in zip(truth_dict_all.items(), header_results_all.items()):\n",
    "    headers_ndcg[true_key] = ndcg_score([true_value], [es_value])\n",
    "    headers_ap[true_key] = average_precision_score(true_value, es_value, pos_label=0)\n",
    "for (true_key, true_value), (es_key, es_value) in zip(truth_dict_all.items(), binary_header_results_all.items()):\n",
    "    headers_f1[true_key] = f1_score(true_value, es_value, zero_division=0, pos_label=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers_eval = pd.DataFrame(data=[headers_ndcg, headers_ap, headers_f1]).T\n",
    "headers_eval = headers_eval.rename({0:\"NDCG\", 1:\"AP\", 2:\"F1\"}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (14,10))\n",
    "sns.kdeplot(headers_eval.NDCG, alpha=.1, shade=True, label='NDCG')\n",
    "sns.kdeplot(headers_eval.AP, alpha=.1, shade=True,label='AP')\n",
    "sns.kdeplot(headers_eval.F1,  alpha=.1, shade=True,label='F1')\n",
    "plt.legend()\n",
    "plt.title('Ranking Metric Performance, Headers Index', fontsize=20)\n",
    "plt.xlabel('Score Distribution', fontsize=16)\n",
    "plt.ylabel('Score Density', fontsize=16)\n",
    "plt.xlim(0,1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which comments were scored poorly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"comments2018.json\") as f:\n",
    "    comments2018 = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poorf1 = headers_eval.query(\"F1 < 0.5\").F1\n",
    "poorf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poorndcg = headers_eval.query(\"NDCG < 0.3 & NDCG > 0\").NDCG\n",
    "poorndcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poorf1_comments = {key:comment for key, comment in comments2018.items() if key in poorf1}\n",
    "{key:len(comment) for key, comment in poorf1_comments.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poorf1_comments_truth = {key:matches for key, matches in truth_dict.items() if key in poorf1}\n",
    "poorf1_comments_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poorndcg_comments = {key:comment for key, comment in comments2018.items() if key in poorndcg}\n",
    "{key:len(comment) for key, comment in poorndcg_comments.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poorndcg_comments_truth = {key:matches for key, matches in truth_dict.items() if key in poorndcg}\n",
    "poorndcg_comments_truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Headers Unique Index Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('unique_header_results.json') as f:\n",
    "    unique_header_results_json = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tranlate ES keys to true rule section names, MAINTAINING ORDER\n",
    "for key, value in unique_header_results_json.items():\n",
    "    value_list = []\n",
    "    for val_idx in value:\n",
    "        for true_key in truth_keys:\n",
    "            if true_key in val_idx and true_key not in value_list: \n",
    "                value_list.append(true_key)\n",
    "        unique_header_results_json[key]=value_list     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create truth dict with values equal to sparse array length=29, binary\n",
    "unique_truth_dict = {key:value for key, value in truth_dict.items() if key in unique_header_results_json}\n",
    "for key, value in unique_truth_dict.items():\n",
    "    new_value_list = np.zeros(len(truth_keys))\n",
    "    for val in value:\n",
    "        for i in range(len(truth_keys)):\n",
    "            if val == truth_keys[i]:\n",
    "                new_value_list[i] = 1\n",
    "    unique_truth_dict[key] = new_value_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create header results dict with values equal to sparse array length=29, (discounted)\n",
    "unique_header_results_all = unique_header_results_json.copy()\n",
    "for key, value in unique_header_results_json.items():\n",
    "    new_value_list = np.zeros(len(truth_keys))\n",
    "    counter = 1\n",
    "    for val in value:\n",
    "        for i in range(len(truth_keys)):\n",
    "            if val == truth_keys[i]:\n",
    "                counter += 1\n",
    "                new_value_list[i] = 1/math.log(counter, 2)\n",
    "    unique_header_results_all[key] = new_value_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create header results dict with values equal to sparse array length=29, binary\n",
    "unique_binary_header_results_all = unique_header_results_json.copy()\n",
    "for key, value in unique_header_results_json.items():\n",
    "    new_value_list = np.zeros(len(truth_keys))\n",
    "    counter = 1\n",
    "    for val in value:\n",
    "        for i in range(len(truth_keys)):\n",
    "            if val == truth_keys[i]:\n",
    "                counter += 1\n",
    "                new_value_list[i] = 1\n",
    "    unique_binary_header_results_all[key] = new_value_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_headers_ndcg = {}\n",
    "unique_headers_ap = {}\n",
    "unique_headers_f1 = {}\n",
    "for (true_key, true_value), (es_key, es_value) in zip(truth_dict_all.items(), unique_header_results_all.items()):\n",
    "    unique_headers_ndcg[true_key] = ndcg_score([true_value], [es_value])\n",
    "    unique_headers_ap[true_key] = average_precision_score(true_value, es_value, pos_label=0)\n",
    "for (true_key, true_value), (es_key, es_value) in zip(truth_dict_all.items(), unique_binary_header_results_all.items()):\n",
    "    unique_headers_f1[true_key] = f1_score(true_value, es_value, zero_division=0, pos_label=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_headers_eval = pd.DataFrame(data=[unique_headers_ndcg, unique_headers_ap, unique_headers_f1]).T\n",
    "unique_headers_eval = unique_headers_eval.rename({0:\"NDCG\", 1:\"AP\", 2:\"F1\"}, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid Index Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hybrid_results.json') as f1:\n",
    "    hybrid_results_json = json.load(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tranlate ES keys to true rule section names, MAINTAINING ORDER\n",
    "for key, value in hybrid_results_json.items():\n",
    "    value_list = []\n",
    "    for val_idx in value:\n",
    "        for true_key in truth_keys:\n",
    "            if true_key in val_idx[0:4] and true_key not in value_list:\n",
    "                    value_list.append(true_key)\n",
    "        hybrid_results_json[key]=value_list            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match_dict shows all true positives, without penalty to false categorization\n",
    "match_dict= {}\n",
    "for (key1, value1), (key2, value2) in zip(truth_dict.items(), hybrid_results_json.items()):\n",
    "    match_count = 0\n",
    "    for val_idx in value1:\n",
    "            if val_idx in value2:\n",
    "                match_count += 1\n",
    "            match_dict[key1]=match_count/len(value1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpenalized accuracy of true positives\n",
    "meanvals = np.mean(list(match_dict.values()))\n",
    "meanvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match_dict1 shows all true positives, with penalty to false categorization\n",
    "match_dict1= {}\n",
    "for (key1, value1), (key2,value2) in zip(truth_dict.items(), hybrid_results_json.items()):\n",
    "    match_count = 0\n",
    "    for val_idx in value2:\n",
    "        if val_idx in value1:\n",
    "            match_count += 1\n",
    "        match_dict1[key1]=match_count/len(value2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# penalized accuracy of true positives\n",
    "meanvals1 = np.mean(list(match_dict1.values()))\n",
    "meanvals1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([meanvals,meanvals1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SKlearn NDCG Hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create header results dict with values equal to sparse array length=29, binary\n",
    "binary_hybrid_results_all = hybrid_results_json.copy()\n",
    "for key, value in hybrid_results_json.items():\n",
    "    new_value_list = np.zeros(len(truth_keys))\n",
    "    for val in value:\n",
    "        for i in range(len(truth_keys)):\n",
    "            if val == truth_keys[i]:\n",
    "                new_value_list[i] = 1\n",
    "    binary_hybrid_results_all[key] = new_value_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create header results dict with values equal to sparse array length=29, binary (discounted)\n",
    "hybrid_results_all = hybrid_results_json.copy()\n",
    "for key, value in hybrid_results_json.items():\n",
    "    new_value_list = np.zeros(len(truth_keys))\n",
    "    counter = 1\n",
    "    for val in value:\n",
    "        for i in range(len(truth_keys)):\n",
    "            if val == truth_keys[i]:\n",
    "                counter += 1\n",
    "                new_value_list[i] = 1/math.log(counter, 2)\n",
    "    hybrid_results_all[key] = new_value_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_ndcg = {}\n",
    "hybrid_ap = {}\n",
    "hybrid_f1 = {}\n",
    "for (true_key, true_value), (es_key, es_value) in zip(truth_dict_all.items(), hybrid_results_all.items()):\n",
    "    hybrid_ndcg[true_key] = ndcg_score([true_value], [es_value])\n",
    "    hybrid_ap[true_key] = average_precision_score(true_value, es_value, pos_label=0)\n",
    "for (true_key, true_value), (es_key, es_value) in zip(truth_dict_all.items(), binary_hybrid_results_all.items()):\n",
    "    hybrid_f1[true_key] = f1_score(true_value, es_value, zero_division=0, pos_label=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_eval = pd.DataFrame(data=[hybrid_ndcg, hybrid_ap, hybrid_f1]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_eval = hybrid_eval.rename({0:\"NDCG\", 1:\"AP\", 2:\"F1\"}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hybrid_mean_scores = hybrid_eval.mean()\n",
    "Hybrid_mean_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (14,10))\n",
    "sns.kdeplot(hybrid_eval.NDCG, alpha=.1, shade=True, label='NDCG')\n",
    "sns.kdeplot(hybrid_eval.AP, alpha=.1, shade=True,label='AP')\n",
    "sns.kdeplot(hybrid_eval.F1,  alpha=.1, shade=True,label='F1')\n",
    "plt.legend()\n",
    "plt.title('Ranking Metric Performance, Hybrid Index', fontsize=20)\n",
    "plt.xlabel('Score Distribution', fontsize=16)\n",
    "plt.ylabel('Score Density', fontsize=16)\n",
    "plt.xlim(0,1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which comments were scored poorly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poorf1 = hybrid_eval.query(\"F1 < 0.5\").F1\n",
    "poorf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poorndcg = hybrid_eval.query(\"NDCG < 0.3 & NDCG > 0\").NDCG\n",
    "poorndcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poorf1_comments = {key:comment for key, comment in comments2018.items() if key in poorf1}\n",
    "{key:len(comment) for key, comment in poorf1_comments.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poorf1_comments_truth = {key:matches for key, matches in truth_dict.items() if key in poorf1}\n",
    "poorf1_comments_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poorndcg_comments = {key:comment for key, comment in comments2018.items() if key in poorndcg}\n",
    "{key:len(comment) for key, comment in poorndcg_comments.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poorndcg_comments_truth = {key:matches for key, matches in truth_dict.items() if key in poorndcg}\n",
    "poorndcg_comments_truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NDCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cummulative_gain(truth, pred):\n",
    "    counter = 0\n",
    "    for i in truth:\n",
    "        if i in pred:\n",
    "            counter +=1\n",
    "    return counter\n",
    "def Discounted_Cummulative_gain(truth, pred):\n",
    "    DCG = 0\n",
    "    for i,j in enumerate(pred):\n",
    "        if j in truth:\n",
    "            DCG += (1/(math.log(i+2,2)))\n",
    "    return DCG\n",
    "def Ideal_discounted_cummulative_gain(truth, pred):\n",
    "    counter = 0\n",
    "    IDCG = 0\n",
    "    for i in truth:\n",
    "        if i in pred:\n",
    "            counter +=1\n",
    "    for i in range(counter):\n",
    "        IDCG += (1/(math.log(i+2,2)))\n",
    "    return IDCG\n",
    "def nDiscounted_Cummulative_gain(truth, pred):\n",
    "    x = Ideal_discounted_cummulative_gain(truth, pred)\n",
    "    y = Discounted_Cummulative_gain(truth, pred)\n",
    "    return y/(x+0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_precision(truth, pred):\n",
    "    runn = 0\n",
    "    cum_gain = 0\n",
    "    for i, j in enumerate(truth):\n",
    "        if j in pred:\n",
    "            cum_gain +=1\n",
    "            runn = runn + cum_gain/(i+1)\n",
    "    average_pred = runn / (cum_gain + 0.0001)\n",
    "    return average_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F_1_score(truth, pred):\n",
    "    AP = average_precision(truth, pred)\n",
    "    F_1 = AP/(AP + 1)\n",
    "    return 2*F_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Headers Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CG = {}\n",
    "DCG = {}\n",
    "iDCG = {}\n",
    "nDCG = {}\n",
    "AP = {}\n",
    "F_1 = {}\n",
    "MAP = 0\n",
    "count = 0\n",
    "for (i,(k,j)) in zip(truth_dict.values(),header_results_json.items()):\n",
    "    CG[k] = Cummulative_gain(i,j)\n",
    "    DCG[k] = Discounted_Cummulative_gain(i,j)\n",
    "    iDCG[k] = Ideal_discounted_cummulative_gain(i,j)\n",
    "    nDCG[k] = nDiscounted_Cummulative_gain(i,j)\n",
    "    AP[k] = average_precision(i,j)\n",
    "    F_1[k] = F_1_score(i,j)\n",
    "    count +=1\n",
    "    MAP +=AP[k]\n",
    "Model_eval = pd.DataFrame(data=[CG, DCG, iDCG, nDCG, AP, F_1])\n",
    "MAP/count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_eval = Model_eval.transpose().rename({0:'CG',1:'DCG',2:'iDCG',3:'nDCG',4:'AP',5:'F1'}, axis=1)\n",
    "Model_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mean_scores = Model_eval.mean()\n",
    "Mean_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (14,10))\n",
    "\n",
    "sns.kdeplot(Model_eval.nDCG, alpha=.1, shade=True, label='nDCG')\n",
    "sns.kdeplot(Model_eval.AP, alpha=.1, shade=True,label='AP')\n",
    "sns.kdeplot(Model_eval.F1,  alpha=.1, shade=True,label='F1')\n",
    "plt.legend()\n",
    "plt.title('Ranking Metric Performance, Headers Index', fontsize=20)\n",
    "plt.xlabel('Score Distribution', fontsize=16)\n",
    "plt.ylabel('Score Density', fontsize=16)\n",
    "plt.xlim(0,1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CG = {}\n",
    "DCG = {}\n",
    "iDCG = {}\n",
    "nDCG = {}\n",
    "AP = {}\n",
    "F_1 = {}\n",
    "MAP = 0\n",
    "count = 0\n",
    "for (i,(k,j)) in zip(truth_dict.values(),hybrid_results_json.items()):\n",
    "    CG[k] = Cummulative_gain(i,j)\n",
    "    DCG[k] = Discounted_Cummulative_gain(i,j)\n",
    "    iDCG[k] = Ideal_discounted_cummulative_gain(i,j)\n",
    "    nDCG[k] = nDiscounted_Cummulative_gain(i,j)\n",
    "    AP[k] = average_precision(i,j)\n",
    "    F_1[k] = F_1_score(i,j)\n",
    "    count += 1\n",
    "    MAP += AP[k]\n",
    "Model_eval2 = pd.DataFrame(data=[CG, DCG, iDCG, nDCG, AP, F_1])\n",
    "MAP/count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_eval2 = Model_eval2.transpose().rename({0:'CG',1:'DCG',2:'iDCG',3:'nDCG',4:'AP',5:'F1'}, axis=1)\n",
    "Model_eval2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mean_scores2 = Model_eval2.mean()\n",
    "Mean_scores2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_df = pd.DataFrame(data=[Mean_scores, Mean_scores2])\n",
    "mean_df=mean_df.rename({0:'headers',1:'hybrid'},axis=0)\n",
    "mean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (14,10))\n",
    "\n",
    "sns.kdeplot(Model_eval2.nDCG, alpha=.1, shade=True, label='nDCG')\n",
    "sns.kdeplot(Model_eval2.AP, alpha=.1, shade=True, label='AP')\n",
    "sns.kdeplot(Model_eval2.F1,  alpha=.1, shade=True, label='F1')\n",
    "plt.legend()\n",
    "plt.title('Ranking Metric Performance, Hybrid Index', fontsize=20)\n",
    "plt.xlabel('Score Distribution', fontsize=16)\n",
    "plt.ylabel('Score Density', fontsize=16)\n",
    "plt.xlim(0,1);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
