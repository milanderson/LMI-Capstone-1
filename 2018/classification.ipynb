{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import string\n",
    "from sklearn.metrics import f1_score as f1, confusion_matrix as confusion, plot_roc_curve as roc\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, TweetTokenizer\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import xgboost as xgb\n",
    "from xgboost import plot_importance\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in Comments and True Classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"unique_comments2018.json\") as f:\n",
    "    texts = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = {key:value.replace(\"\\r\", \" \").replace(\"\\n\", \" \").replace(\"\\s\", \" \") for key, value in texts.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in texts.items():\n",
    "    texts[key] = ''.join(c for c in value if c in string.printable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_url = \"https://mikeanders.org/data/CMS/CMS-2018-0101-0001/CMS-1701-P%20Comment%20MetaData.csv\"\n",
    "data = pd.read_csv(metadata_url, usecols=range(0,36))[:468] #ignore last few columns and blank rows at end of csv \n",
    "data = data.rename(columns=lambda x: x.strip()) #strip whitespace from columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data10 = data.fillna(0) #fill NaN with 0\n",
    "section_cols = data10.columns[3:] \n",
    "data10[section_cols] = data10[section_cols].replace([\"Y\"], 1) #replace Y with 1 in approriate columns\n",
    "data11 = data10.copy()\n",
    "section_cols1  = data11.columns[3:] \n",
    "data11[section_cols1] = np.where((data11[section_cols1]  != 1),0,data11[section_cols1] )\n",
    "\n",
    "# Combining columns for index matching: (A6b, A6b.1, = A6b),  (C3b, C3b.1'= C3b) ('A7', 'A7.1', 'A7.2', = A7b, a7c),  (F = F2, F3)\n",
    "data11['A6b'] = (data11['A6b'] + data11['A6b.1'])\n",
    "data11['A6b'] = data11['A6b'].replace(2,1)\n",
    "data11['C3b'] = (data11['C3b'] + data11['C3b.1'])\n",
    "data11['C3b'] = data11['C3b'].replace(2,1)\n",
    "data11['A7'] = (data11['A7'] + data11['A7.1'] + data11['A7.2'])\n",
    "data11['A7'] = data11['A7'].replace(2,1)\n",
    "data11['A7'] = data11['A7'].replace(3,1)\n",
    "data11 = data11.drop(['A6b.1', 'C3b.1', 'A7.1', 'A7.2'], axis = 1)\n",
    "\n",
    "data11.Name = [name.split('DRAFT-')[1].split('-')[0] for name in data11.Name]\n",
    "data11 = data11.rename(columns=lambda x: x.lower())\n",
    "section_cols1 = data11.columns[3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data11 = data11.loc[data11['name'].isin(texts.keys())]\n",
    "data11[\"comment\"] = texts.values() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data11, test_size=0.2, random_state=44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a2      27\n",
       "a3      38\n",
       "a4b      3\n",
       "a4c     24\n",
       "a5b     27\n",
       "a5c     14\n",
       "a5d     13\n",
       "a6b      9\n",
       "a6c      8\n",
       "a6d2     2\n",
       "a6d3     6\n",
       "a7      19\n",
       "b2a     22\n",
       "b2b     21\n",
       "c2      23\n",
       "c3a     16\n",
       "c3b     13\n",
       "d2      34\n",
       "d3b     23\n",
       "d3c     16\n",
       "d3d      3\n",
       "d4       1\n",
       "e2      14\n",
       "e3       6\n",
       "e4       5\n",
       "e5      20\n",
       "e6      16\n",
       "e7       7\n",
       "f        2\n",
       "dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.sum(axis=0)[section_cols1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = list(train.comment)\n",
    "test_texts = list(test.comment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify One Rule Section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stopwords = ['!', '\"', \"#\", \"$\", \"%\", \"&\", \"(\", \")\", \"*\", \"+\", \",\", \"-\", \".\", \"/\", \":\", \";\", \"<\", \"=\", \">\", \"?\", \"@\", \"[\", \"]\", \"^\", \"_\", \"`\", \"{\", \"|\", \"}\", \"~\", \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vector = CountVectorizer(tokenizer=nltk.RegexpTokenizer(r\"\\w+\").tokenize, ngram_range=(1,1), stop_words='english')\n",
    "\n",
    "x_train = bow_vector.fit_transform(train_texts)\n",
    "y_train = np.array(train.a2)\n",
    "\n",
    "x_test = bow_vector.transform(test_texts)\n",
    "y_test = np.array(test.a2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm = SGDClassifier(random_state=44)\n",
    "svm.fit(X=x_train, y=y_train)\n",
    "svm_preds = svm.predict(x_test)\n",
    "svm_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3636363636363636"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_f1 = f1(y_test, svm_preds)\n",
    "svm_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10,  4],\n",
       "       [ 3,  2]], dtype=int64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_confusion = confusion(y_test, svm_preds)\n",
    "svm_confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identifying Most Significant Words for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['telehealth',\n",
       " 'phd',\n",
       " 'success',\n",
       " 'trend',\n",
       " 'county',\n",
       " 'agreement',\n",
       " 'spending',\n",
       " 'period',\n",
       " 'track',\n",
       " 'uhg',\n",
       " 'texas',\n",
       " 'adjustment',\n",
       " 'acos',\n",
       " 'risk',\n",
       " 'regional']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefs = svm.coef_[0]\n",
    "top_fifteen = np.argpartition(coefs, -15)[-15:]\n",
    "[(bow_vector.get_feature_names()[feature]) for feature in top_fifteen]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "boost = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "boost.fit(x_train, y_train)\n",
    "boost_preds = boost.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3636363636363636"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boost_f1 = f1(y_test, svm_preds)\n",
    "boost_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10,  4],\n",
       "       [ 2,  3]], dtype=int64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boost_confusion = confusion(y_test, boost_preds)\n",
    "boost_confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identifying Most Significant Words for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['significant',\n",
       " 'administrative',\n",
       " 'choosing',\n",
       " 'flexibility',\n",
       " '8',\n",
       " 'llc',\n",
       " 'access',\n",
       " 'nursing',\n",
       " 'challenges',\n",
       " '1',\n",
       " 'e',\n",
       " 'good',\n",
       " 'primary',\n",
       " 'aco',\n",
       " 'attached']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boost_top15 = np.argsort(-boost.feature_importances_)[0:15]\n",
    "[(bow_vector.get_feature_names()[feature]) for feature in boost_top15] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify All Rule Sections - TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_tokenize(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    stems = [PorterStemmer().stem(item) for item in tokens]\n",
    "    return stems\n",
    "\n",
    "tfidf_vector = TfidfVectorizer(tokenizer=stem_tokenize, ngram_range=(1,1), max_df=0.9)\n",
    "\n",
    "x_train = tfidf_vector.fit_transform(train_texts)\n",
    "y_train = np.array(train[section_cols1])\n",
    "\n",
    "x_test = tfidf_vector.transform(test_texts)\n",
    "y_test = np.array(test[section_cols1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "boost = xgb.XGBClassifier(objective='binary:logistic', use_label_encoder=False, eval_metric='logloss')\n",
    "\n",
    "multi_boost = MultiOutputClassifier(boost)\n",
    "\n",
    "multi_boost.fit(x_train, y_train)\n",
    "\n",
    "multi_boost_preds = multi_boost.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_boost_f1 = f1(y_test, multi_boost_preds, zero_division=0, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a2': 0.6666666666666665,\n",
       " 'a3': 0.6666666666666666,\n",
       " 'a4b': 0.0,\n",
       " 'a4c': 0.5,\n",
       " 'a5b': 0.888888888888889,\n",
       " 'a5c': 0.8,\n",
       " 'a5d': 0.8,\n",
       " 'a6b': 0.8,\n",
       " 'a6c': 0.0,\n",
       " 'a6d2': 0.0,\n",
       " 'a6d3': 0.0,\n",
       " 'a7': 0.5714285714285715,\n",
       " 'b2a': 0.6,\n",
       " 'b2b': 1.0,\n",
       " 'c2': 0.5,\n",
       " 'c3a': 0.6666666666666666,\n",
       " 'c3b': 0.6666666666666666,\n",
       " 'd2': 0.8000000000000002,\n",
       " 'd3b': 0.8571428571428571,\n",
       " 'd3c': 0.0,\n",
       " 'd3d': 0.0,\n",
       " 'd4': 0.0,\n",
       " 'e2': 0.0,\n",
       " 'e3': 0.0,\n",
       " 'e4': 0.0,\n",
       " 'e5': 0.6,\n",
       " 'e6': 0.8571428571428571,\n",
       " 'e7': 0.0,\n",
       " 'f': 0.0}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = {sec:score for (sec, score) in zip(section_cols1, list(multi_boost_f1))}\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42211275314723595"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(multi_boost_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identifying Most Significant Words for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "for boost in multi_boost.estimators_:\n",
    "    boost_top10 = np.argsort(-boost.feature_importances_)[0:10]\n",
    "    features.append([(tfidf_vector.get_feature_names()[feature]) for feature in boost_top10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'boost_features' (dict)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'a2': (0.6666666666666665,\n",
       "  ['these',\n",
       "   'choos',\n",
       "   '20201',\n",
       "   'goal',\n",
       "   'access',\n",
       "   'next',\n",
       "   '4',\n",
       "   'thi',\n",
       "   'while',\n",
       "   '$']),\n",
       " 'a3': (0.625,\n",
       "  ['what',\n",
       "   'score',\n",
       "   'percent',\n",
       "   'revenu',\n",
       "   'ensur',\n",
       "   'would',\n",
       "   'oper',\n",
       "   'depart',\n",
       "   'box',\n",
       "   'an']),\n",
       " 'a4b': (0.0,\n",
       "  ['cms1701p',\n",
       "   'decemb',\n",
       "   'msr/mlr',\n",
       "   'templat',\n",
       "   'materi',\n",
       "   '60',\n",
       "   'cours',\n",
       "   'treat',\n",
       "   'must',\n",
       "   'invest']),\n",
       " 'a4c': (0.6,\n",
       "  ['waiver',\n",
       "   'promot',\n",
       "   'expand',\n",
       "   'less',\n",
       "   'fewer',\n",
       "   'permit',\n",
       "   'behalf',\n",
       "   'attach',\n",
       "   'prospect',\n",
       "   'concern']),\n",
       " 'a5b': (0.888888888888889,\n",
       "  ['revenu',\n",
       "   '#',\n",
       "   'reason',\n",
       "   'both',\n",
       "   'could',\n",
       "   'encourag',\n",
       "   'should',\n",
       "   'and',\n",
       "   'than',\n",
       "   'attach']),\n",
       " 'a5c': (0.8,\n",
       "  ['outsid',\n",
       "   'corridor',\n",
       "   'gener',\n",
       "   'determin',\n",
       "   'experi',\n",
       "   'requir',\n",
       "   'pleas',\n",
       "   'renew',\n",
       "   'and',\n",
       "   '12']),\n",
       " 'a5d': (0.6666666666666666,\n",
       "  ['outsid', 'agre', \"''\", '10', ':', 'do', 'work', 's', ')', 'control']),\n",
       " 'a6b': (0.5,\n",
       "  ['msr/mlr',\n",
       "   'msr',\n",
       "   'term',\n",
       "   'am',\n",
       "   'attach',\n",
       "   '(',\n",
       "   '30',\n",
       "   ',',\n",
       "   'and',\n",
       "   'coordin']),\n",
       " 'a6c': (0.0,\n",
       "  ['applic',\n",
       "   'mechan',\n",
       "   '``',\n",
       "   'letter',\n",
       "   'fund',\n",
       "   'submit',\n",
       "   'care',\n",
       "   'through',\n",
       "   'like',\n",
       "   'oper']),\n",
       " 'a6d2': (0.0,\n",
       "  ['0',\n",
       "   'phone',\n",
       "   'letter',\n",
       "   \"''\",\n",
       "   'aco',\n",
       "   '(',\n",
       "   'as',\n",
       "   'attent',\n",
       "   '!',\n",
       "   'organizationspathway']),\n",
       " 'a6d3': (0.0,\n",
       "  ['msr',\n",
       "   'msr/mlr',\n",
       "   'termin',\n",
       "   'earli',\n",
       "   'applic',\n",
       "   '1.',\n",
       "   'an',\n",
       "   'public',\n",
       "   'with',\n",
       "   ')']),\n",
       " 'a7': (0.0,\n",
       "  ['januari',\n",
       "   'longer',\n",
       "   '31',\n",
       "   '(',\n",
       "   'choic',\n",
       "   'have',\n",
       "   'extrem',\n",
       "   ':',\n",
       "   'healthcar',\n",
       "   'attach']),\n",
       " 'b2a': (0.8000000000000002,\n",
       "  ['waiver',\n",
       "   'redesign',\n",
       "   'recommend',\n",
       "   'nurs',\n",
       "   'associ',\n",
       "   'assign',\n",
       "   'number',\n",
       "   '$',\n",
       "   '(',\n",
       "   'afford']),\n",
       " 'b2b': (0.8571428571428571,\n",
       "  ['telehealth',\n",
       "   'expenditur',\n",
       "   'termin',\n",
       "   'associ',\n",
       "   'support',\n",
       "   'am',\n",
       "   '&',\n",
       "   'thi',\n",
       "   'methodolog',\n",
       "   'attach']),\n",
       " 'c2': (0.5,\n",
       "  ['expand',\n",
       "   'snf',\n",
       "   'octob',\n",
       "   'preliminari',\n",
       "   '?',\n",
       "   'receiv',\n",
       "   'determin',\n",
       "   'after',\n",
       "   'increment',\n",
       "   'serv']),\n",
       " 'c3a': (0.6666666666666666,\n",
       "  ['employ',\n",
       "   'notif',\n",
       "   'permit',\n",
       "   'approxim',\n",
       "   'but',\n",
       "   'letter',\n",
       "   'use',\n",
       "   'and',\n",
       "   'commit',\n",
       "   'unnecessari']),\n",
       " 'c3b': (0.8571428571428571,\n",
       "  ['rather',\n",
       "   'opt-in',\n",
       "   'voluntarili',\n",
       "   'medicar',\n",
       "   'an',\n",
       "   '42',\n",
       "   'capit',\n",
       "   'advantag',\n",
       "   'align',\n",
       "   'enrol']),\n",
       " 'd2': (0.8000000000000002,\n",
       "  ['implement',\n",
       "   'letter',\n",
       "   'adjust',\n",
       "   'benchmark',\n",
       "   'year',\n",
       "   \"''\",\n",
       "   'potenti',\n",
       "   'risk',\n",
       "   'expand',\n",
       "   '16']),\n",
       " 'd3b': (0.7499999999999999,\n",
       "  ['incent',\n",
       "   'either',\n",
       "   '-',\n",
       "   'determin',\n",
       "   'and',\n",
       "   'transit',\n",
       "   '4',\n",
       "   'associ',\n",
       "   'enhanc',\n",
       "   'percent']),\n",
       " 'd3c': (0.0,\n",
       "  ['2', 'region', '50', 'fall', '1', 'spend', 'medicaid', '1.', 'up', 'see']),\n",
       " 'd3d': (0.0,\n",
       "  ['2017.',\n",
       "   '3.',\n",
       "   'aggreg',\n",
       "   'blend',\n",
       "   'find',\n",
       "   'shift',\n",
       "   'profit',\n",
       "   'hope',\n",
       "   'thu',\n",
       "   'benchmark']),\n",
       " 'd4': (0.0,\n",
       "  ['+/-',\n",
       "   '2019',\n",
       "   'a',\n",
       "   'either',\n",
       "   'criteria',\n",
       "   'these',\n",
       "   'urg',\n",
       "   'data',\n",
       "   '&',\n",
       "   \"'s\"]),\n",
       " 'e2': (0.6666666666666666,\n",
       "  ['voluntari',\n",
       "   'million',\n",
       "   'fax',\n",
       "   'ha',\n",
       "   'primari',\n",
       "   'beneficiari',\n",
       "   '1.',\n",
       "   'clinician',\n",
       "   'associ',\n",
       "   'a']),\n",
       " 'e3': (0.4,\n",
       "  ['accord',\n",
       "   'inexperienc',\n",
       "   'percent',\n",
       "   '31',\n",
       "   '(',\n",
       "   'certif',\n",
       "   'function',\n",
       "   'accur',\n",
       "   'simpli',\n",
       "   'enter']),\n",
       " 'e4': (0.0,\n",
       "  ['cycl',\n",
       "   'abus',\n",
       "   '10',\n",
       "   'corridor',\n",
       "   'differ',\n",
       "   '2019',\n",
       "   'downsid',\n",
       "   'less',\n",
       "   'level',\n",
       "   ',']),\n",
       " 'e5': (0.6,\n",
       "  ['diabet',\n",
       "   'measur',\n",
       "   'enabl',\n",
       "   'abus',\n",
       "   'treatment',\n",
       "   'pleas',\n",
       "   'outcom',\n",
       "   'comprehens',\n",
       "   'cms-1701-p',\n",
       "   'factor']),\n",
       " 'e6': (0.8571428571428571,\n",
       "  ['cehrt',\n",
       "   'behalf',\n",
       "   'work',\n",
       "   'gradual',\n",
       "   'relat',\n",
       "   'attach',\n",
       "   'payment',\n",
       "   \"'s\",\n",
       "   '1.',\n",
       "   'pathway']),\n",
       " 'e7': (0.0,\n",
       "  ['advers',\n",
       "   'sponsor',\n",
       "   'technolog',\n",
       "   'total',\n",
       "   'file',\n",
       "   'burden',\n",
       "   'see',\n",
       "   'use',\n",
       "   'relat',\n",
       "   'attach']),\n",
       " 'f': (0.0,\n",
       "  ['clarif',\n",
       "   'need',\n",
       "   'my',\n",
       "   'current',\n",
       "   'understand',\n",
       "   'basic',\n",
       "   'and',\n",
       "   'under',\n",
       "   'medicar',\n",
       "   'as'])}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "important_features = {sec:feature_list for (sec, feature_list) in zip(section_cols1, features)}\n",
    "boost_features = {key:(value,features) for (key, value), (key1, features) in zip(scores.items(), important_features.items())}\n",
    "\n",
    "%store boost_features\n",
    "boost_features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
